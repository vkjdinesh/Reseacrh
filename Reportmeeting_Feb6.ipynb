{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1aMc-hVkVJP_nr7HDpIrh-e40HhOi58dc",
      "authorship_tag": "ABX9TyN1caOgum9TFIRb1bpVghRf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkjdinesh/Reseacrh/blob/main/Reportmeeting_Feb6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from socket import socket\n",
        "import seaborn as sns\n",
        "#library\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import matplotlib.pyplot as plt \n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "import numpy\n",
        "import numpy as np\n",
        "from numpy import cov\n",
        "from numpy import trace\n",
        "from numpy import iscomplexobj\n",
        "from numpy import asarray\n",
        "from numpy.random import shuffle\n",
        "from scipy.linalg import sqrtm\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.datasets.mnist import load_data\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "import  scipy\n",
        "import numpy as np\n",
        "from scipy.linalg import sqrtm\n",
        "import cmath"
      ],
      "metadata": {
        "id": "7bITMEoQsIjF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3y-L3qGNiLyI"
      },
      "outputs": [],
      "source": [
        "#select the specific digits\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Filter for a specific digit\n",
        "digit_to_filter = 5\n",
        "x_train = x_train[y_train == digit_to_filter]\n",
        "y_train = y_train[y_train == digit_to_filter]\n",
        "x_test= x_test[y_test == digit_to_filter]\n",
        "y_test = y_test[y_test == digit_to_filter]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mnist\n",
        "import mnist\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdTQdQMkshzr",
        "outputId": "2c011180-c076-4c6f-f292-027887929a98"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mnist in /usr/local/lib/python3.8/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mnist) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = mnist.train_images() \n",
        "train_labels = mnist.train_labels()\n",
        "test_images = mnist.test_images()\n",
        "test_labels = mnist.test_labels()"
      ],
      "metadata": {
        "id": "Xn0werQlsc4t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "digitDict = {}\n",
        "\n",
        "for i in range(10):\n",
        "    mask = (train_labels == i)\n",
        "    digitDict[i] = train_images[mask]\n",
        "\n",
        "for i in range(10):\n",
        "    print(\"Digit {0} matrix shape: {1}\".format(i,digitDict[i].shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzuq8ROdsTjh",
        "outputId": "1444c1f0-d050-46a1-8524-221a71e74e33"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digit 0 matrix shape: (5923, 28, 28)\n",
            "Digit 1 matrix shape: (6742, 28, 28)\n",
            "Digit 2 matrix shape: (5958, 28, 28)\n",
            "Digit 3 matrix shape: (6131, 28, 28)\n",
            "Digit 4 matrix shape: (5842, 28, 28)\n",
            "Digit 5 matrix shape: (5421, 28, 28)\n",
            "Digit 6 matrix shape: (5918, 28, 28)\n",
            "Digit 7 matrix shape: (6265, 28, 28)\n",
            "Digit 8 matrix shape: (5851, 28, 28)\n",
            "Digit 9 matrix shape: (5949, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#labels\n",
        "some_digit = x_train[10]\n",
        "some_digit_show = plt.imshow(x_train[10].reshape(28,28))\n",
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "7r9YmYdHieRP",
        "outputId": "712f4685-7b82-4bf9-b0f3-f352e544921f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5421, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MNIST-DCGAN-FID\n",
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class DCGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generates imgs\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        valid = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, valid)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
        "        model.add(Reshape((7, 7, 128)))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
        "        model.add(Activation(\"tanh\"))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=128, save_interval=50):\n",
        "        import tensorflow as tf\n",
        "        # Load the dataset\n",
        "        #(X_train, _), (_, _) = mnist.load_data()\n",
        "        # Load the MNIST dataset\n",
        "        mnist = tf.keras.datasets.mnist\n",
        "        (X_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        # Filter for a specific digit\n",
        "        digit_to_filter = 5\n",
        "        X_train = X_train[y_train == digit_to_filter]\n",
        "        y_train = y_train[y_train == digit_to_filter]\n",
        "        x_test= x_test[y_test == digit_to_filter]\n",
        "        y_test = y_test[y_test == digit_to_filter]\n",
        "\n",
        "        # Rescale -1 to 1\n",
        "        X_train = X_train / 127.5 - 1.\n",
        "        X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random half of images\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "\n",
        "            # Sample noise and generate a batch of new images\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator (real classified as ones and generated as zeros)\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Train the generator (wants discriminator to mistake images as real)\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "\n",
        "            # Plot the progress\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % save_interval == 0:\n",
        "                self.save_imgs(epoch)\n",
        "    def save_imgs(self, epoch):\n",
        "        r, c = 2,2\n",
        "        #noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
        "        plt.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "3cLiMHcyso2Z"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    dcgan = DCGAN()\n",
        "    dcgan.train(epochs=1000, batch_size=32, save_interval=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuV52a8gtNrC",
        "outputId": "00e0ed22-45e0-4c57-903c-6d65f230885d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_7 (Conv2D)           (None, 14, 14, 32)        320       \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 14, 14, 32)        0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 7, 7, 64)          18496     \n",
            "                                                                 \n",
            " zero_padding2d_1 (ZeroPaddi  (None, 8, 8, 64)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 8, 8, 64)         256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 4, 4, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 4, 4, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 4, 4, 256)         295168    \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 4, 4, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 4097      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 393,729\n",
            "Trainable params: 392,833\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 6272)              633472    \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " up_sampling2d_2 (UpSampling  (None, 14, 14, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 14, 14, 128)       147584    \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 14, 14, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " up_sampling2d_3 (UpSampling  (None, 28, 28, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 28, 28, 64)        73792     \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 28, 28, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 28, 28, 64)        0         \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 28, 28, 1)         577       \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 856,193\n",
            "Trainable params: 855,809\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 0s 189ms/step\n",
            "0 [D loss: 1.229639, acc.: 45.31%] [G loss: 0.606024]\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1 [D loss: 0.495427, acc.: 70.31%] [G loss: 0.580972]\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "2 [D loss: 0.207206, acc.: 96.88%] [G loss: 0.616262]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "3 [D loss: 0.185646, acc.: 96.88%] [G loss: 0.655446]\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "4 [D loss: 0.131996, acc.: 98.44%] [G loss: 0.686894]\n",
            "1/1 [==============================] - 0s 201ms/step\n",
            "5 [D loss: 0.214000, acc.: 90.62%] [G loss: 0.668328]\n",
            "1/1 [==============================] - 0s 217ms/step\n",
            "6 [D loss: 0.180525, acc.: 96.88%] [G loss: 0.798165]\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "7 [D loss: 0.128892, acc.: 96.88%] [G loss: 0.803012]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "8 [D loss: 0.064833, acc.: 100.00%] [G loss: 0.770229]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "9 [D loss: 0.079492, acc.: 100.00%] [G loss: 0.729295]\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "10 [D loss: 0.050782, acc.: 100.00%] [G loss: 0.450998]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "11 [D loss: 0.021109, acc.: 100.00%] [G loss: 0.353567]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "12 [D loss: 0.016803, acc.: 100.00%] [G loss: 0.339929]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "13 [D loss: 0.014477, acc.: 100.00%] [G loss: 0.222214]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "14 [D loss: 0.007102, acc.: 100.00%] [G loss: 0.106878]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "15 [D loss: 0.012322, acc.: 100.00%] [G loss: 0.082080]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "16 [D loss: 0.029401, acc.: 100.00%] [G loss: 0.056603]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "17 [D loss: 0.065126, acc.: 100.00%] [G loss: 0.101412]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "18 [D loss: 0.141700, acc.: 96.88%] [G loss: 0.232442]\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "19 [D loss: 0.498269, acc.: 70.31%] [G loss: 1.120674]\n",
            "1/1 [==============================] - 0s 228ms/step\n",
            "20 [D loss: 1.365282, acc.: 37.50%] [G loss: 2.754459]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 219ms/step\n",
            "21 [D loss: 1.038095, acc.: 50.00%] [G loss: 1.180013]\n",
            "1/1 [==============================] - 0s 220ms/step\n",
            "22 [D loss: 0.284841, acc.: 90.62%] [G loss: 0.425866]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "23 [D loss: 0.063899, acc.: 100.00%] [G loss: 0.226909]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "24 [D loss: 0.045865, acc.: 100.00%] [G loss: 0.284395]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "25 [D loss: 0.028940, acc.: 100.00%] [G loss: 0.246531]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "26 [D loss: 0.169389, acc.: 95.31%] [G loss: 0.394479]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "27 [D loss: 0.534989, acc.: 71.88%] [G loss: 1.374789]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "28 [D loss: 1.485127, acc.: 37.50%] [G loss: 2.629951]\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "29 [D loss: 1.364262, acc.: 35.94%] [G loss: 2.105842]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "30 [D loss: 1.004685, acc.: 43.75%] [G loss: 1.236364]\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "31 [D loss: 0.730477, acc.: 60.94%] [G loss: 1.153850]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "32 [D loss: 0.892205, acc.: 54.69%] [G loss: 1.306789]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "33 [D loss: 0.680706, acc.: 67.19%] [G loss: 1.222157]\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "34 [D loss: 0.830562, acc.: 57.81%] [G loss: 1.074846]\n",
            "1/1 [==============================] - 0s 223ms/step\n",
            "35 [D loss: 0.879479, acc.: 51.56%] [G loss: 1.489067]\n",
            "1/1 [==============================] - 0s 192ms/step\n",
            "36 [D loss: 0.760447, acc.: 57.81%] [G loss: 1.638359]\n",
            "1/1 [==============================] - 0s 207ms/step\n",
            "37 [D loss: 0.910422, acc.: 51.56%] [G loss: 0.882354]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "38 [D loss: 0.702186, acc.: 68.75%] [G loss: 0.748784]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "39 [D loss: 0.696429, acc.: 64.06%] [G loss: 0.849119]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "40 [D loss: 0.883224, acc.: 56.25%] [G loss: 1.250479]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "41 [D loss: 0.887370, acc.: 56.25%] [G loss: 1.156488]\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "42 [D loss: 1.111152, acc.: 43.75%] [G loss: 1.309948]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "43 [D loss: 0.837749, acc.: 54.69%] [G loss: 1.090753]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "44 [D loss: 0.792273, acc.: 54.69%] [G loss: 1.459680]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "45 [D loss: 0.750558, acc.: 50.00%] [G loss: 1.484433]\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "46 [D loss: 0.875706, acc.: 51.56%] [G loss: 1.122243]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "47 [D loss: 0.652133, acc.: 67.19%] [G loss: 1.113443]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "48 [D loss: 0.500099, acc.: 73.44%] [G loss: 1.383791]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "49 [D loss: 0.865394, acc.: 46.88%] [G loss: 1.214772]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "50 [D loss: 0.761021, acc.: 59.38%] [G loss: 1.131152]\n",
            "1/1 [==============================] - 0s 214ms/step\n",
            "51 [D loss: 0.561422, acc.: 68.75%] [G loss: 0.804644]\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "52 [D loss: 0.629130, acc.: 68.75%] [G loss: 0.936516]\n",
            "1/1 [==============================] - 0s 224ms/step\n",
            "53 [D loss: 0.592811, acc.: 64.06%] [G loss: 1.299117]\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "54 [D loss: 0.684004, acc.: 62.50%] [G loss: 1.397098]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "55 [D loss: 1.290802, acc.: 34.38%] [G loss: 0.837525]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "56 [D loss: 1.053959, acc.: 43.75%] [G loss: 1.085879]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "57 [D loss: 0.550438, acc.: 71.88%] [G loss: 1.173162]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "58 [D loss: 0.753243, acc.: 57.81%] [G loss: 1.264048]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "59 [D loss: 0.737433, acc.: 64.06%] [G loss: 1.192830]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "60 [D loss: 0.513896, acc.: 76.56%] [G loss: 0.977371]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "61 [D loss: 0.295678, acc.: 90.62%] [G loss: 0.753772]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "62 [D loss: 0.287854, acc.: 85.94%] [G loss: 0.654831]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "63 [D loss: 0.254333, acc.: 92.19%] [G loss: 0.616892]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "64 [D loss: 0.410437, acc.: 82.81%] [G loss: 1.077290]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "65 [D loss: 0.847294, acc.: 46.88%] [G loss: 1.411390]\n",
            "1/1 [==============================] - 0s 190ms/step\n",
            "66 [D loss: 0.835333, acc.: 54.69%] [G loss: 1.380641]\n",
            "1/1 [==============================] - 0s 225ms/step\n",
            "67 [D loss: 0.677320, acc.: 65.62%] [G loss: 1.241191]\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "68 [D loss: 0.766323, acc.: 56.25%] [G loss: 0.760926]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "69 [D loss: 0.516927, acc.: 70.31%] [G loss: 0.946661]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "70 [D loss: 0.493962, acc.: 73.44%] [G loss: 1.499637]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "71 [D loss: 0.565713, acc.: 75.00%] [G loss: 1.627088]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "72 [D loss: 0.576716, acc.: 73.44%] [G loss: 1.264690]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "73 [D loss: 0.667988, acc.: 59.38%] [G loss: 1.185188]\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "74 [D loss: 0.474260, acc.: 76.56%] [G loss: 1.375620]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "75 [D loss: 0.630988, acc.: 68.75%] [G loss: 1.686386]\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "76 [D loss: 0.549307, acc.: 71.88%] [G loss: 2.068570]\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "77 [D loss: 0.565717, acc.: 70.31%] [G loss: 1.471269]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "78 [D loss: 0.632415, acc.: 70.31%] [G loss: 1.261525]\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "79 [D loss: 0.424408, acc.: 82.81%] [G loss: 1.080048]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "80 [D loss: 0.578207, acc.: 67.19%] [G loss: 1.106505]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 202ms/step\n",
            "81 [D loss: 0.617939, acc.: 68.75%] [G loss: 1.580466]\n",
            "1/1 [==============================] - 0s 197ms/step\n",
            "82 [D loss: 0.827934, acc.: 51.56%] [G loss: 1.736036]\n",
            "1/1 [==============================] - 0s 205ms/step\n",
            "83 [D loss: 0.536979, acc.: 75.00%] [G loss: 1.387716]\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "84 [D loss: 0.577501, acc.: 70.31%] [G loss: 1.183105]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "85 [D loss: 0.448960, acc.: 76.56%] [G loss: 1.559250]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "86 [D loss: 0.497745, acc.: 71.88%] [G loss: 1.646707]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "87 [D loss: 0.728301, acc.: 57.81%] [G loss: 1.409179]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "88 [D loss: 0.605609, acc.: 70.31%] [G loss: 1.249595]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "89 [D loss: 0.476623, acc.: 76.56%] [G loss: 1.241949]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "90 [D loss: 0.443133, acc.: 75.00%] [G loss: 1.298159]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "91 [D loss: 0.609730, acc.: 62.50%] [G loss: 1.295064]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "92 [D loss: 0.601136, acc.: 64.06%] [G loss: 1.770899]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "93 [D loss: 0.879487, acc.: 60.94%] [G loss: 1.216278]\n",
            "1/1 [==============================] - 0s 426ms/step\n",
            "94 [D loss: 0.601033, acc.: 67.19%] [G loss: 1.115829]\n",
            "1/1 [==============================] - 0s 443ms/step\n",
            "95 [D loss: 0.391061, acc.: 82.81%] [G loss: 1.481789]\n",
            "1/1 [==============================] - 0s 244ms/step\n",
            "96 [D loss: 0.701633, acc.: 67.19%] [G loss: 1.502100]\n",
            "1/1 [==============================] - 0s 235ms/step\n",
            "97 [D loss: 0.572509, acc.: 73.44%] [G loss: 1.501910]\n",
            "1/1 [==============================] - 0s 204ms/step\n",
            "98 [D loss: 0.700531, acc.: 60.94%] [G loss: 1.179757]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "99 [D loss: 0.721284, acc.: 60.94%] [G loss: 1.144084]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "100 [D loss: 0.418302, acc.: 81.25%] [G loss: 1.395146]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "101 [D loss: 0.458125, acc.: 78.12%] [G loss: 1.051737]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "102 [D loss: 0.755879, acc.: 50.00%] [G loss: 1.520410]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "103 [D loss: 0.602803, acc.: 65.62%] [G loss: 1.939225]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "104 [D loss: 0.598256, acc.: 73.44%] [G loss: 1.274640]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "105 [D loss: 0.490657, acc.: 76.56%] [G loss: 1.114586]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "106 [D loss: 0.529370, acc.: 71.88%] [G loss: 0.973225]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "107 [D loss: 0.496181, acc.: 73.44%] [G loss: 1.543497]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "108 [D loss: 0.666114, acc.: 64.06%] [G loss: 1.195717]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "109 [D loss: 0.663092, acc.: 59.38%] [G loss: 1.480646]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "110 [D loss: 0.778623, acc.: 57.81%] [G loss: 1.451521]\n",
            "1/1 [==============================] - 0s 204ms/step\n",
            "111 [D loss: 0.888134, acc.: 54.69%] [G loss: 1.683916]\n",
            "1/1 [==============================] - 0s 214ms/step\n",
            "112 [D loss: 0.692925, acc.: 67.19%] [G loss: 1.458569]\n",
            "1/1 [==============================] - 0s 192ms/step\n",
            "113 [D loss: 0.841169, acc.: 51.56%] [G loss: 1.406305]\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "114 [D loss: 0.651665, acc.: 62.50%] [G loss: 1.476526]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "115 [D loss: 0.570149, acc.: 71.88%] [G loss: 1.244752]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "116 [D loss: 0.551105, acc.: 76.56%] [G loss: 1.233841]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "117 [D loss: 0.597616, acc.: 65.62%] [G loss: 1.454076]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "118 [D loss: 0.711943, acc.: 64.06%] [G loss: 1.453344]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "119 [D loss: 0.563547, acc.: 75.00%] [G loss: 1.352547]\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "120 [D loss: 0.981383, acc.: 48.44%] [G loss: 1.095001]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "121 [D loss: 0.543962, acc.: 73.44%] [G loss: 0.962063]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "122 [D loss: 0.512416, acc.: 75.00%] [G loss: 1.467448]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "123 [D loss: 0.727384, acc.: 59.38%] [G loss: 1.486996]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "124 [D loss: 0.549792, acc.: 73.44%] [G loss: 1.495929]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "125 [D loss: 0.436638, acc.: 76.56%] [G loss: 1.430748]\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "126 [D loss: 0.557594, acc.: 76.56%] [G loss: 1.376922]\n",
            "1/1 [==============================] - 0s 210ms/step\n",
            "127 [D loss: 0.771060, acc.: 56.25%] [G loss: 1.591081]\n",
            "1/1 [==============================] - 0s 212ms/step\n",
            "128 [D loss: 0.522737, acc.: 71.88%] [G loss: 1.340361]\n",
            "1/1 [==============================] - 0s 200ms/step\n",
            "129 [D loss: 0.616514, acc.: 65.62%] [G loss: 1.274571]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "130 [D loss: 0.646434, acc.: 68.75%] [G loss: 1.314655]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "131 [D loss: 0.776849, acc.: 59.38%] [G loss: 1.484864]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "132 [D loss: 0.768281, acc.: 59.38%] [G loss: 0.951347]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "133 [D loss: 0.509436, acc.: 76.56%] [G loss: 0.933613]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "134 [D loss: 0.628906, acc.: 76.56%] [G loss: 1.094589]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "135 [D loss: 0.829090, acc.: 54.69%] [G loss: 0.994870]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "136 [D loss: 0.736022, acc.: 59.38%] [G loss: 1.580045]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "137 [D loss: 0.603936, acc.: 67.19%] [G loss: 1.227584]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "138 [D loss: 0.698971, acc.: 64.06%] [G loss: 1.461235]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "139 [D loss: 0.821416, acc.: 54.69%] [G loss: 1.440043]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "140 [D loss: 0.507279, acc.: 78.12%] [G loss: 1.101506]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "141 [D loss: 0.644854, acc.: 65.62%] [G loss: 1.463671]\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "142 [D loss: 0.802612, acc.: 51.56%] [G loss: 1.733231]\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "143 [D loss: 0.871321, acc.: 50.00%] [G loss: 0.915643]\n",
            "1/1 [==============================] - 0s 200ms/step\n",
            "144 [D loss: 1.001691, acc.: 53.12%] [G loss: 1.169752]\n",
            "1/1 [==============================] - 0s 200ms/step\n",
            "145 [D loss: 0.624534, acc.: 64.06%] [G loss: 1.315342]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "146 [D loss: 0.712090, acc.: 70.31%] [G loss: 1.197027]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "147 [D loss: 0.703696, acc.: 62.50%] [G loss: 1.758318]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "148 [D loss: 0.596813, acc.: 67.19%] [G loss: 1.535899]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "149 [D loss: 0.576792, acc.: 68.75%] [G loss: 1.148760]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "150 [D loss: 0.767949, acc.: 56.25%] [G loss: 1.242037]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "151 [D loss: 0.592155, acc.: 70.31%] [G loss: 1.550220]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "152 [D loss: 0.929038, acc.: 54.69%] [G loss: 1.034148]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "153 [D loss: 0.550224, acc.: 73.44%] [G loss: 1.371405]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "154 [D loss: 0.672773, acc.: 62.50%] [G loss: 1.101744]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "155 [D loss: 0.708736, acc.: 54.69%] [G loss: 1.230870]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "156 [D loss: 0.884616, acc.: 50.00%] [G loss: 1.193108]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "157 [D loss: 1.006393, acc.: 45.31%] [G loss: 1.304264]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "158 [D loss: 0.545823, acc.: 75.00%] [G loss: 1.435909]\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "159 [D loss: 0.692811, acc.: 70.31%] [G loss: 1.132439]\n",
            "1/1 [==============================] - 0s 216ms/step\n",
            "160 [D loss: 0.545306, acc.: 71.88%] [G loss: 1.293864]\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 181ms/step\n",
            "161 [D loss: 0.861319, acc.: 46.88%] [G loss: 1.067970]\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "162 [D loss: 0.640480, acc.: 67.19%] [G loss: 1.272255]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "163 [D loss: 0.600662, acc.: 71.88%] [G loss: 1.229254]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "164 [D loss: 0.679832, acc.: 62.50%] [G loss: 1.295691]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "165 [D loss: 0.864020, acc.: 43.75%] [G loss: 1.227720]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "166 [D loss: 1.001373, acc.: 39.06%] [G loss: 1.437901]\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "167 [D loss: 0.614423, acc.: 65.62%] [G loss: 1.465240]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "168 [D loss: 0.993359, acc.: 48.44%] [G loss: 1.047421]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "169 [D loss: 0.672804, acc.: 56.25%] [G loss: 1.394866]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "170 [D loss: 0.916700, acc.: 48.44%] [G loss: 1.185035]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "171 [D loss: 0.628898, acc.: 71.88%] [G loss: 1.473320]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "172 [D loss: 0.901086, acc.: 51.56%] [G loss: 1.161631]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "173 [D loss: 0.628447, acc.: 67.19%] [G loss: 1.114787]\n",
            "1/1 [==============================] - 0s 206ms/step\n",
            "174 [D loss: 0.600597, acc.: 65.62%] [G loss: 1.221518]\n",
            "1/1 [==============================] - 0s 225ms/step\n",
            "175 [D loss: 0.829770, acc.: 51.56%] [G loss: 1.065236]\n",
            "1/1 [==============================] - 0s 217ms/step\n",
            "176 [D loss: 0.507302, acc.: 73.44%] [G loss: 1.024205]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "177 [D loss: 0.726515, acc.: 65.62%] [G loss: 1.026113]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "178 [D loss: 0.758445, acc.: 57.81%] [G loss: 1.311107]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "179 [D loss: 0.736327, acc.: 68.75%] [G loss: 1.723918]\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "180 [D loss: 0.931060, acc.: 50.00%] [G loss: 1.181724]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "181 [D loss: 0.879412, acc.: 45.31%] [G loss: 1.083323]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "182 [D loss: 0.950010, acc.: 48.44%] [G loss: 1.208212]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "183 [D loss: 0.625482, acc.: 68.75%] [G loss: 1.423337]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "184 [D loss: 0.740949, acc.: 62.50%] [G loss: 1.329802]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "185 [D loss: 0.827816, acc.: 57.81%] [G loss: 1.171617]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "186 [D loss: 0.576444, acc.: 73.44%] [G loss: 1.196583]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "187 [D loss: 0.891191, acc.: 48.44%] [G loss: 0.953455]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "188 [D loss: 0.414882, acc.: 82.81%] [G loss: 1.517956]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "189 [D loss: 1.094718, acc.: 42.19%] [G loss: 1.120240]\n",
            "1/1 [==============================] - 0s 238ms/step\n",
            "190 [D loss: 0.775227, acc.: 62.50%] [G loss: 1.171092]\n",
            "1/1 [==============================] - 0s 220ms/step\n",
            "191 [D loss: 0.716490, acc.: 56.25%] [G loss: 1.143022]\n",
            "1/1 [==============================] - 0s 237ms/step\n",
            "192 [D loss: 0.662393, acc.: 60.94%] [G loss: 0.897035]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "193 [D loss: 0.666820, acc.: 67.19%] [G loss: 1.152249]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "194 [D loss: 0.660298, acc.: 65.62%] [G loss: 0.984611]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "195 [D loss: 0.641693, acc.: 64.06%] [G loss: 1.607490]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "196 [D loss: 0.924734, acc.: 51.56%] [G loss: 1.191142]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "197 [D loss: 0.870642, acc.: 53.12%] [G loss: 1.130158]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "198 [D loss: 0.734060, acc.: 56.25%] [G loss: 1.212788]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "199 [D loss: 0.601010, acc.: 71.88%] [G loss: 1.205468]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "200 [D loss: 0.699618, acc.: 60.94%] [G loss: 1.151706]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "201 [D loss: 0.552294, acc.: 65.62%] [G loss: 1.091819]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "202 [D loss: 0.564758, acc.: 73.44%] [G loss: 1.068212]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "203 [D loss: 0.715572, acc.: 59.38%] [G loss: 1.132289]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "204 [D loss: 0.529017, acc.: 70.31%] [G loss: 1.201752]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "205 [D loss: 0.537847, acc.: 67.19%] [G loss: 1.309154]\n",
            "1/1 [==============================] - 0s 221ms/step\n",
            "206 [D loss: 0.689198, acc.: 60.94%] [G loss: 1.161147]\n",
            "1/1 [==============================] - 0s 220ms/step\n",
            "207 [D loss: 0.848094, acc.: 54.69%] [G loss: 1.083193]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "208 [D loss: 0.760484, acc.: 62.50%] [G loss: 1.335981]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "209 [D loss: 0.739960, acc.: 62.50%] [G loss: 1.186654]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "210 [D loss: 0.586146, acc.: 65.62%] [G loss: 0.889293]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "211 [D loss: 0.628223, acc.: 60.94%] [G loss: 1.003300]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "212 [D loss: 0.860030, acc.: 45.31%] [G loss: 1.089767]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "213 [D loss: 0.677306, acc.: 59.38%] [G loss: 1.130302]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "214 [D loss: 0.931139, acc.: 43.75%] [G loss: 0.970290]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "215 [D loss: 0.623313, acc.: 62.50%] [G loss: 1.341783]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "216 [D loss: 0.862612, acc.: 53.12%] [G loss: 1.100004]\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "217 [D loss: 0.709319, acc.: 67.19%] [G loss: 1.184652]\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "218 [D loss: 0.783093, acc.: 50.00%] [G loss: 1.183377]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "219 [D loss: 0.903940, acc.: 50.00%] [G loss: 1.026135]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "220 [D loss: 0.917003, acc.: 42.19%] [G loss: 1.287833]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 223ms/step\n",
            "221 [D loss: 0.550922, acc.: 68.75%] [G loss: 1.163417]\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "222 [D loss: 0.723487, acc.: 51.56%] [G loss: 1.016270]\n",
            "1/1 [==============================] - 0s 234ms/step\n",
            "223 [D loss: 0.643174, acc.: 64.06%] [G loss: 1.264946]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "224 [D loss: 0.726363, acc.: 56.25%] [G loss: 1.136675]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "225 [D loss: 0.633688, acc.: 68.75%] [G loss: 1.137119]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "226 [D loss: 0.712215, acc.: 64.06%] [G loss: 1.156144]\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "227 [D loss: 0.497306, acc.: 76.56%] [G loss: 1.073117]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "228 [D loss: 0.745771, acc.: 48.44%] [G loss: 1.264537]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "229 [D loss: 0.777388, acc.: 57.81%] [G loss: 1.159828]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "230 [D loss: 0.801930, acc.: 54.69%] [G loss: 1.252753]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "231 [D loss: 0.722067, acc.: 59.38%] [G loss: 1.075790]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "232 [D loss: 0.760658, acc.: 51.56%] [G loss: 1.055810]\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "233 [D loss: 0.544278, acc.: 67.19%] [G loss: 0.915345]\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "234 [D loss: 0.666077, acc.: 64.06%] [G loss: 1.059698]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "235 [D loss: 0.616519, acc.: 67.19%] [G loss: 1.045772]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "236 [D loss: 0.682313, acc.: 65.62%] [G loss: 0.964309]\n",
            "1/1 [==============================] - 0s 195ms/step\n",
            "237 [D loss: 0.799095, acc.: 53.12%] [G loss: 1.112211]\n",
            "1/1 [==============================] - 0s 214ms/step\n",
            "238 [D loss: 0.774093, acc.: 57.81%] [G loss: 1.244153]\n",
            "1/1 [==============================] - 0s 216ms/step\n",
            "239 [D loss: 0.640572, acc.: 65.62%] [G loss: 1.178740]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "240 [D loss: 0.486742, acc.: 71.88%] [G loss: 1.131426]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "241 [D loss: 0.362597, acc.: 85.94%] [G loss: 0.914428]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "242 [D loss: 0.658993, acc.: 60.94%] [G loss: 0.969196]\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "243 [D loss: 0.431191, acc.: 81.25%] [G loss: 1.056509]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "244 [D loss: 0.669738, acc.: 64.06%] [G loss: 0.884837]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "245 [D loss: 0.475305, acc.: 79.69%] [G loss: 0.972605]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "246 [D loss: 0.837805, acc.: 57.81%] [G loss: 0.847053]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "247 [D loss: 0.658639, acc.: 67.19%] [G loss: 1.149497]\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "248 [D loss: 0.937772, acc.: 43.75%] [G loss: 0.951803]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "249 [D loss: 0.561028, acc.: 73.44%] [G loss: 0.802351]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "250 [D loss: 0.833690, acc.: 53.12%] [G loss: 1.046040]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "251 [D loss: 0.671079, acc.: 65.62%] [G loss: 1.208052]\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "252 [D loss: 0.824173, acc.: 53.12%] [G loss: 0.996956]\n",
            "1/1 [==============================] - 0s 206ms/step\n",
            "253 [D loss: 0.599834, acc.: 71.88%] [G loss: 1.229987]\n",
            "1/1 [==============================] - 0s 251ms/step\n",
            "254 [D loss: 0.556571, acc.: 68.75%] [G loss: 0.568629]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "255 [D loss: 0.350814, acc.: 89.06%] [G loss: 0.654029]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "256 [D loss: 0.359893, acc.: 87.50%] [G loss: 0.857704]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "257 [D loss: 0.342322, acc.: 82.81%] [G loss: 0.996076]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "258 [D loss: 0.495103, acc.: 71.88%] [G loss: 0.956770]\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "259 [D loss: 0.607996, acc.: 64.06%] [G loss: 1.240834]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "260 [D loss: 0.700333, acc.: 54.69%] [G loss: 1.224296]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "261 [D loss: 0.803007, acc.: 54.69%] [G loss: 1.265111]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "262 [D loss: 0.464206, acc.: 81.25%] [G loss: 1.315736]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "263 [D loss: 0.864044, acc.: 51.56%] [G loss: 1.113821]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "264 [D loss: 0.630109, acc.: 64.06%] [G loss: 1.167943]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "265 [D loss: 0.484159, acc.: 75.00%] [G loss: 1.037261]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "266 [D loss: 0.478215, acc.: 79.69%] [G loss: 0.574908]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "267 [D loss: 0.359513, acc.: 87.50%] [G loss: 0.692941]\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "268 [D loss: 0.567291, acc.: 76.56%] [G loss: 0.923255]\n",
            "1/1 [==============================] - 0s 210ms/step\n",
            "269 [D loss: 0.413549, acc.: 76.56%] [G loss: 0.905218]\n",
            "1/1 [==============================] - 0s 208ms/step\n",
            "270 [D loss: 0.344259, acc.: 85.94%] [G loss: 1.121867]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "271 [D loss: 0.738860, acc.: 59.38%] [G loss: 1.258246]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "272 [D loss: 0.467033, acc.: 76.56%] [G loss: 1.271554]\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "273 [D loss: 0.806037, acc.: 51.56%] [G loss: 1.011111]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "274 [D loss: 0.459553, acc.: 76.56%] [G loss: 0.899410]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "275 [D loss: 0.583750, acc.: 75.00%] [G loss: 0.821080]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "276 [D loss: 0.678247, acc.: 68.75%] [G loss: 1.136680]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "277 [D loss: 0.752836, acc.: 62.50%] [G loss: 1.282653]\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "278 [D loss: 0.411405, acc.: 82.81%] [G loss: 1.181036]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "279 [D loss: 0.677161, acc.: 62.50%] [G loss: 1.008938]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "280 [D loss: 0.573783, acc.: 65.62%] [G loss: 0.863253]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "281 [D loss: 0.598732, acc.: 65.62%] [G loss: 1.126084]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "282 [D loss: 0.539316, acc.: 73.44%] [G loss: 1.227281]\n",
            "1/1 [==============================] - 0s 210ms/step\n",
            "283 [D loss: 0.487143, acc.: 71.88%] [G loss: 0.827523]\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "284 [D loss: 0.344274, acc.: 84.38%] [G loss: 0.719143]\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "285 [D loss: 0.664925, acc.: 59.38%] [G loss: 1.091120]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "286 [D loss: 0.464890, acc.: 79.69%] [G loss: 1.297031]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "287 [D loss: 0.668360, acc.: 64.06%] [G loss: 1.081713]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "288 [D loss: 0.391264, acc.: 82.81%] [G loss: 1.248064]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "289 [D loss: 0.672137, acc.: 65.62%] [G loss: 1.031286]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "290 [D loss: 0.632531, acc.: 62.50%] [G loss: 1.450886]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "291 [D loss: 0.789759, acc.: 59.38%] [G loss: 1.144820]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "292 [D loss: 0.851023, acc.: 53.12%] [G loss: 1.265350]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "293 [D loss: 0.583110, acc.: 76.56%] [G loss: 1.341317]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "294 [D loss: 0.909005, acc.: 53.12%] [G loss: 1.045016]\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "295 [D loss: 0.546311, acc.: 76.56%] [G loss: 1.070443]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "296 [D loss: 0.559109, acc.: 62.50%] [G loss: 0.892834]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "297 [D loss: 0.637294, acc.: 64.06%] [G loss: 0.796622]\n",
            "1/1 [==============================] - 0s 193ms/step\n",
            "298 [D loss: 0.635840, acc.: 70.31%] [G loss: 1.232695]\n",
            "1/1 [==============================] - 0s 209ms/step\n",
            "299 [D loss: 0.579838, acc.: 68.75%] [G loss: 0.968829]\n",
            "1/1 [==============================] - 0s 204ms/step\n",
            "300 [D loss: 0.615761, acc.: 71.88%] [G loss: 0.922146]\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "301 [D loss: 0.529557, acc.: 76.56%] [G loss: 1.297374]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "302 [D loss: 0.630486, acc.: 65.62%] [G loss: 0.965474]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "303 [D loss: 0.771926, acc.: 57.81%] [G loss: 0.963426]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "304 [D loss: 0.572470, acc.: 71.88%] [G loss: 1.232976]\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "305 [D loss: 0.865093, acc.: 46.88%] [G loss: 0.962327]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "306 [D loss: 0.514617, acc.: 71.88%] [G loss: 1.405343]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "307 [D loss: 0.750113, acc.: 57.81%] [G loss: 0.924216]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "308 [D loss: 0.399483, acc.: 81.25%] [G loss: 1.133582]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "309 [D loss: 0.568176, acc.: 71.88%] [G loss: 1.091385]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "310 [D loss: 0.494501, acc.: 75.00%] [G loss: 0.992054]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "311 [D loss: 0.314918, acc.: 87.50%] [G loss: 0.959241]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "312 [D loss: 0.587391, acc.: 75.00%] [G loss: 1.014740]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "313 [D loss: 0.507160, acc.: 78.12%] [G loss: 0.977592]\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "314 [D loss: 0.592106, acc.: 75.00%] [G loss: 1.326471]\n",
            "1/1 [==============================] - 0s 226ms/step\n",
            "315 [D loss: 0.563251, acc.: 67.19%] [G loss: 1.233167]\n",
            "1/1 [==============================] - 0s 234ms/step\n",
            "316 [D loss: 0.856143, acc.: 46.88%] [G loss: 1.375215]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "317 [D loss: 0.485268, acc.: 73.44%] [G loss: 1.513871]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "318 [D loss: 0.599140, acc.: 70.31%] [G loss: 1.094193]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "319 [D loss: 0.590677, acc.: 71.88%] [G loss: 0.812276]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "320 [D loss: 0.608686, acc.: 67.19%] [G loss: 0.910819]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "321 [D loss: 0.376873, acc.: 81.25%] [G loss: 1.012028]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "322 [D loss: 0.433965, acc.: 79.69%] [G loss: 1.116569]\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "323 [D loss: 0.513490, acc.: 71.88%] [G loss: 1.164641]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "324 [D loss: 0.684309, acc.: 65.62%] [G loss: 0.775573]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "325 [D loss: 0.469291, acc.: 75.00%] [G loss: 1.006436]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "326 [D loss: 0.463129, acc.: 78.12%] [G loss: 1.433178]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "327 [D loss: 0.757476, acc.: 57.81%] [G loss: 1.520210]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "328 [D loss: 0.886656, acc.: 42.19%] [G loss: 1.400538]\n",
            "1/1 [==============================] - 0s 214ms/step\n",
            "329 [D loss: 0.518013, acc.: 71.88%] [G loss: 1.346912]\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "330 [D loss: 0.589453, acc.: 70.31%] [G loss: 1.341575]\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "331 [D loss: 0.349835, acc.: 87.50%] [G loss: 1.215679]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "332 [D loss: 0.456787, acc.: 76.56%] [G loss: 1.100020]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "333 [D loss: 0.359122, acc.: 85.94%] [G loss: 0.959940]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "334 [D loss: 0.439167, acc.: 81.25%] [G loss: 0.776198]\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "335 [D loss: 0.865660, acc.: 59.38%] [G loss: 1.154390]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "336 [D loss: 0.694994, acc.: 60.94%] [G loss: 1.485690]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "337 [D loss: 0.676960, acc.: 65.62%] [G loss: 1.325408]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "338 [D loss: 1.255600, acc.: 35.94%] [G loss: 1.498168]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "339 [D loss: 0.528572, acc.: 65.62%] [G loss: 1.264660]\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "340 [D loss: 0.660173, acc.: 68.75%] [G loss: 1.409524]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "341 [D loss: 0.525197, acc.: 82.81%] [G loss: 1.129777]\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "342 [D loss: 0.526003, acc.: 76.56%] [G loss: 1.305677]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "343 [D loss: 0.360640, acc.: 85.94%] [G loss: 1.217893]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "344 [D loss: 0.616068, acc.: 64.06%] [G loss: 1.067190]\n",
            "1/1 [==============================] - 0s 225ms/step\n",
            "345 [D loss: 0.334992, acc.: 87.50%] [G loss: 1.077059]\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "346 [D loss: 0.421986, acc.: 82.81%] [G loss: 1.100228]\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "347 [D loss: 0.445234, acc.: 76.56%] [G loss: 0.974596]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "348 [D loss: 0.339475, acc.: 81.25%] [G loss: 1.276100]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "349 [D loss: 0.673090, acc.: 67.19%] [G loss: 0.855798]\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "350 [D loss: 0.314113, acc.: 90.62%] [G loss: 1.059347]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "351 [D loss: 0.365468, acc.: 87.50%] [G loss: 1.272562]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "352 [D loss: 0.519759, acc.: 79.69%] [G loss: 0.933582]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "353 [D loss: 0.540374, acc.: 68.75%] [G loss: 1.113877]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "354 [D loss: 0.521074, acc.: 76.56%] [G loss: 1.302897]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "355 [D loss: 0.314589, acc.: 87.50%] [G loss: 1.143762]\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "356 [D loss: 0.478836, acc.: 84.38%] [G loss: 1.161137]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "357 [D loss: 0.337631, acc.: 85.94%] [G loss: 1.061430]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "358 [D loss: 0.646130, acc.: 64.06%] [G loss: 0.847970]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "359 [D loss: 0.450139, acc.: 81.25%] [G loss: 1.034587]\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "360 [D loss: 0.265685, acc.: 93.75%] [G loss: 1.033360]\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 206ms/step\n",
            "361 [D loss: 0.544219, acc.: 78.12%] [G loss: 1.110872]\n",
            "1/1 [==============================] - 0s 208ms/step\n",
            "362 [D loss: 0.413148, acc.: 82.81%] [G loss: 1.110539]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "363 [D loss: 0.669140, acc.: 65.62%] [G loss: 1.276636]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "364 [D loss: 0.345899, acc.: 87.50%] [G loss: 1.211047]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "365 [D loss: 0.474706, acc.: 73.44%] [G loss: 1.111450]\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "366 [D loss: 0.762017, acc.: 56.25%] [G loss: 0.918543]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "367 [D loss: 0.149386, acc.: 95.31%] [G loss: 1.132859]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "368 [D loss: 0.603428, acc.: 71.88%] [G loss: 0.933419]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "369 [D loss: 0.442609, acc.: 78.12%] [G loss: 0.900026]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "370 [D loss: 0.217625, acc.: 95.31%] [G loss: 1.043630]\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "371 [D loss: 0.307287, acc.: 89.06%] [G loss: 1.224931]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "372 [D loss: 0.237998, acc.: 93.75%] [G loss: 0.947543]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "373 [D loss: 0.351638, acc.: 87.50%] [G loss: 1.515935]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "374 [D loss: 0.300996, acc.: 95.31%] [G loss: 1.405578]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "375 [D loss: 0.355775, acc.: 85.94%] [G loss: 1.421770]\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "376 [D loss: 0.653815, acc.: 57.81%] [G loss: 1.138556]\n",
            "1/1 [==============================] - 0s 204ms/step\n",
            "377 [D loss: 0.653056, acc.: 64.06%] [G loss: 1.149287]\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "378 [D loss: 0.349377, acc.: 85.94%] [G loss: 1.214370]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "379 [D loss: 0.661606, acc.: 68.75%] [G loss: 1.497411]\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "380 [D loss: 0.783194, acc.: 54.69%] [G loss: 1.104884]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "381 [D loss: 0.134662, acc.: 98.44%] [G loss: 0.964629]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "382 [D loss: 0.212166, acc.: 92.19%] [G loss: 1.043329]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "383 [D loss: 0.221696, acc.: 93.75%] [G loss: 1.045451]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "384 [D loss: 0.442862, acc.: 81.25%] [G loss: 1.138584]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "385 [D loss: 0.570979, acc.: 73.44%] [G loss: 0.874161]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "386 [D loss: 0.250724, acc.: 90.62%] [G loss: 0.837273]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "387 [D loss: 0.551572, acc.: 70.31%] [G loss: 1.542232]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "388 [D loss: 0.771234, acc.: 57.81%] [G loss: 1.256345]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "389 [D loss: 0.464788, acc.: 76.56%] [G loss: 1.145030]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "390 [D loss: 0.527361, acc.: 73.44%] [G loss: 1.620644]\n",
            "1/1 [==============================] - 0s 222ms/step\n",
            "391 [D loss: 1.187998, acc.: 34.38%] [G loss: 1.261427]\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "392 [D loss: 1.077545, acc.: 39.06%] [G loss: 1.222455]\n",
            "1/1 [==============================] - 0s 195ms/step\n",
            "393 [D loss: 0.578546, acc.: 71.88%] [G loss: 1.114417]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "394 [D loss: 1.766238, acc.: 12.50%] [G loss: 1.268391]\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "395 [D loss: 0.501360, acc.: 70.31%] [G loss: 1.391988]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "396 [D loss: 1.033541, acc.: 37.50%] [G loss: 1.232801]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "397 [D loss: 0.546932, acc.: 76.56%] [G loss: 1.173641]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "398 [D loss: 0.494489, acc.: 71.88%] [G loss: 0.953109]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "399 [D loss: 0.369868, acc.: 84.38%] [G loss: 0.861757]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "400 [D loss: 0.197313, acc.: 93.75%] [G loss: 1.045602]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "401 [D loss: 0.189843, acc.: 95.31%] [G loss: 0.814417]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "402 [D loss: 0.353884, acc.: 85.94%] [G loss: 1.018788]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "403 [D loss: 0.223697, acc.: 92.19%] [G loss: 1.073850]\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "404 [D loss: 0.350535, acc.: 89.06%] [G loss: 1.067447]\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "405 [D loss: 0.201202, acc.: 96.88%] [G loss: 1.038838]\n",
            "1/1 [==============================] - 0s 238ms/step\n",
            "406 [D loss: 0.720050, acc.: 57.81%] [G loss: 1.396414]\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "407 [D loss: 1.084375, acc.: 43.75%] [G loss: 1.102592]\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "408 [D loss: 1.053812, acc.: 31.25%] [G loss: 1.129482]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "409 [D loss: 0.275973, acc.: 92.19%] [G loss: 1.291408]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "410 [D loss: 0.758279, acc.: 57.81%] [G loss: 1.246987]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "411 [D loss: 0.277727, acc.: 93.75%] [G loss: 1.058344]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "412 [D loss: 0.433968, acc.: 81.25%] [G loss: 1.309816]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "413 [D loss: 0.247487, acc.: 93.75%] [G loss: 1.287530]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "414 [D loss: 0.271089, acc.: 92.19%] [G loss: 1.014901]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "415 [D loss: 0.353256, acc.: 82.81%] [G loss: 1.080413]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "416 [D loss: 0.322574, acc.: 87.50%] [G loss: 1.067805]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "417 [D loss: 0.316385, acc.: 89.06%] [G loss: 0.875464]\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "418 [D loss: 0.313476, acc.: 92.19%] [G loss: 1.062076]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "419 [D loss: 0.301533, acc.: 89.06%] [G loss: 1.296194]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "420 [D loss: 0.767021, acc.: 59.38%] [G loss: 0.817711]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "421 [D loss: 0.328780, acc.: 85.94%] [G loss: 1.346890]\n",
            "1/1 [==============================] - 0s 209ms/step\n",
            "422 [D loss: 0.589130, acc.: 67.19%] [G loss: 0.995040]\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "423 [D loss: 1.620542, acc.: 14.06%] [G loss: 1.138585]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "424 [D loss: 0.603036, acc.: 68.75%] [G loss: 1.198359]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "425 [D loss: 0.527448, acc.: 70.31%] [G loss: 1.184448]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "426 [D loss: 0.366772, acc.: 89.06%] [G loss: 1.139821]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "427 [D loss: 0.346763, acc.: 85.94%] [G loss: 1.069542]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "428 [D loss: 0.105954, acc.: 98.44%] [G loss: 1.036392]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "429 [D loss: 0.346564, acc.: 87.50%] [G loss: 1.053504]\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "430 [D loss: 0.578310, acc.: 71.88%] [G loss: 0.949814]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "431 [D loss: 0.102053, acc.: 100.00%] [G loss: 1.234992]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "432 [D loss: 0.221133, acc.: 93.75%] [G loss: 0.849998]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "433 [D loss: 0.197906, acc.: 90.62%] [G loss: 1.008134]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "434 [D loss: 0.431117, acc.: 82.81%] [G loss: 1.332134]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "435 [D loss: 0.209753, acc.: 95.31%] [G loss: 1.492139]\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "436 [D loss: 0.291426, acc.: 89.06%] [G loss: 1.262230]\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "437 [D loss: 0.328267, acc.: 89.06%] [G loss: 1.151220]\n",
            "1/1 [==============================] - 0s 225ms/step\n",
            "438 [D loss: 0.657625, acc.: 67.19%] [G loss: 1.353528]\n",
            "1/1 [==============================] - 0s 207ms/step\n",
            "439 [D loss: 0.258960, acc.: 90.62%] [G loss: 1.209472]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "440 [D loss: 0.286378, acc.: 93.75%] [G loss: 1.273425]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "441 [D loss: 0.395446, acc.: 79.69%] [G loss: 1.233846]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "442 [D loss: 0.191570, acc.: 98.44%] [G loss: 0.993345]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "443 [D loss: 0.266437, acc.: 92.19%] [G loss: 1.050680]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "444 [D loss: 0.092919, acc.: 98.44%] [G loss: 1.112068]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "445 [D loss: 0.294212, acc.: 93.75%] [G loss: 0.867135]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "446 [D loss: 1.091853, acc.: 26.56%] [G loss: 0.836638]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "447 [D loss: 0.154320, acc.: 100.00%] [G loss: 1.162274]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "448 [D loss: 0.446588, acc.: 81.25%] [G loss: 0.859017]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "449 [D loss: 0.138339, acc.: 96.88%] [G loss: 1.019345]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "450 [D loss: 0.662893, acc.: 64.06%] [G loss: 1.315050]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "451 [D loss: 1.534997, acc.: 18.75%] [G loss: 0.904687]\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "452 [D loss: 0.341149, acc.: 84.38%] [G loss: 1.095143]\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "453 [D loss: 0.417939, acc.: 82.81%] [G loss: 1.042864]\n",
            "1/1 [==============================] - 0s 204ms/step\n",
            "454 [D loss: 0.535293, acc.: 73.44%] [G loss: 1.057409]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "455 [D loss: 0.341260, acc.: 89.06%] [G loss: 1.026869]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "456 [D loss: 0.254035, acc.: 92.19%] [G loss: 0.792700]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "457 [D loss: 0.184532, acc.: 92.19%] [G loss: 0.824440]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "458 [D loss: 0.174676, acc.: 96.88%] [G loss: 1.021170]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "459 [D loss: 0.052278, acc.: 100.00%] [G loss: 1.032815]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "460 [D loss: 0.389668, acc.: 78.12%] [G loss: 1.321687]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "461 [D loss: 0.459642, acc.: 70.31%] [G loss: 1.028437]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "462 [D loss: 0.320973, acc.: 81.25%] [G loss: 0.964054]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "463 [D loss: 0.095965, acc.: 98.44%] [G loss: 1.324097]\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "464 [D loss: 0.193626, acc.: 92.19%] [G loss: 1.196824]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "465 [D loss: 0.296784, acc.: 87.50%] [G loss: 1.216838]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "466 [D loss: 0.199835, acc.: 95.31%] [G loss: 1.011084]\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "467 [D loss: 0.522149, acc.: 73.44%] [G loss: 1.012270]\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "468 [D loss: 0.764979, acc.: 59.38%] [G loss: 1.041779]\n",
            "1/1 [==============================] - 0s 222ms/step\n",
            "469 [D loss: 0.368348, acc.: 84.38%] [G loss: 1.053048]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "470 [D loss: 1.621027, acc.: 35.94%] [G loss: 1.141593]\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "471 [D loss: 0.868080, acc.: 54.69%] [G loss: 1.392718]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "472 [D loss: 0.460204, acc.: 78.12%] [G loss: 1.327307]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "473 [D loss: 0.517535, acc.: 71.88%] [G loss: 0.828301]\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "474 [D loss: 0.257363, acc.: 90.62%] [G loss: 0.934801]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "475 [D loss: 0.064170, acc.: 100.00%] [G loss: 0.938612]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "476 [D loss: 0.279612, acc.: 93.75%] [G loss: 0.880135]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "477 [D loss: 0.038020, acc.: 100.00%] [G loss: 0.831596]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "478 [D loss: 0.041069, acc.: 100.00%] [G loss: 0.679363]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "479 [D loss: 0.132155, acc.: 95.31%] [G loss: 0.686445]\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "480 [D loss: 0.162892, acc.: 96.88%] [G loss: 0.822356]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "481 [D loss: 0.129888, acc.: 98.44%] [G loss: 0.811789]\n",
            "1/1 [==============================] - 0s 222ms/step\n",
            "482 [D loss: 0.158690, acc.: 95.31%] [G loss: 0.652857]\n",
            "1/1 [==============================] - 0s 205ms/step\n",
            "483 [D loss: 0.044496, acc.: 98.44%] [G loss: 1.251443]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "484 [D loss: 0.074301, acc.: 98.44%] [G loss: 1.128856]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "485 [D loss: 0.129424, acc.: 96.88%] [G loss: 0.728945]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "486 [D loss: 0.036849, acc.: 100.00%] [G loss: 1.086114]\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "487 [D loss: 0.048220, acc.: 100.00%] [G loss: 0.727561]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "488 [D loss: 0.167963, acc.: 95.31%] [G loss: 0.806227]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "489 [D loss: 0.089176, acc.: 100.00%] [G loss: 0.990143]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "490 [D loss: 0.360740, acc.: 81.25%] [G loss: 0.861683]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "491 [D loss: 0.316885, acc.: 89.06%] [G loss: 0.926539]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "492 [D loss: 1.175151, acc.: 43.75%] [G loss: 0.982726]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "493 [D loss: 0.137787, acc.: 93.75%] [G loss: 0.922565]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "494 [D loss: 0.308062, acc.: 85.94%] [G loss: 0.923693]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "495 [D loss: 0.813720, acc.: 60.94%] [G loss: 1.383670]\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "496 [D loss: 0.294051, acc.: 89.06%] [G loss: 1.786209]\n",
            "1/1 [==============================] - 0s 217ms/step\n",
            "497 [D loss: 0.443191, acc.: 81.25%] [G loss: 1.004719]\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "498 [D loss: 0.131098, acc.: 96.88%] [G loss: 1.035962]\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "499 [D loss: 0.130896, acc.: 96.88%] [G loss: 0.911662]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "500 [D loss: 0.052159, acc.: 100.00%] [G loss: 0.825924]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "501 [D loss: 0.031965, acc.: 100.00%] [G loss: 0.830098]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "502 [D loss: 0.109190, acc.: 96.88%] [G loss: 0.729463]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "503 [D loss: 0.031149, acc.: 100.00%] [G loss: 1.079394]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "504 [D loss: 0.241178, acc.: 90.62%] [G loss: 0.816375]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "505 [D loss: 0.261313, acc.: 84.38%] [G loss: 0.791240]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "506 [D loss: 0.023465, acc.: 100.00%] [G loss: 1.146019]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "507 [D loss: 0.177539, acc.: 95.31%] [G loss: 1.322662]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "508 [D loss: 0.017756, acc.: 100.00%] [G loss: 1.530860]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "509 [D loss: 0.006918, acc.: 100.00%] [G loss: 0.872739]\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "510 [D loss: 0.082710, acc.: 96.88%] [G loss: 0.591100]\n",
            "1/1 [==============================] - 0s 199ms/step\n",
            "511 [D loss: 0.039732, acc.: 98.44%] [G loss: 1.145959]\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "512 [D loss: 0.035001, acc.: 100.00%] [G loss: 1.117791]\n",
            "1/1 [==============================] - 0s 211ms/step\n",
            "513 [D loss: 0.109699, acc.: 98.44%] [G loss: 0.913056]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "514 [D loss: 0.073935, acc.: 100.00%] [G loss: 0.977698]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "515 [D loss: 0.109156, acc.: 98.44%] [G loss: 1.210876]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "516 [D loss: 0.488335, acc.: 76.56%] [G loss: 0.966594]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "517 [D loss: 0.076061, acc.: 98.44%] [G loss: 1.041112]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "518 [D loss: 0.067080, acc.: 100.00%] [G loss: 0.948089]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "519 [D loss: 0.073231, acc.: 98.44%] [G loss: 1.005422]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "520 [D loss: 0.176745, acc.: 96.88%] [G loss: 1.049418]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "521 [D loss: 0.054989, acc.: 100.00%] [G loss: 0.977525]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "522 [D loss: 0.476653, acc.: 78.12%] [G loss: 1.140252]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "523 [D loss: 0.153840, acc.: 98.44%] [G loss: 1.148436]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "524 [D loss: 0.252433, acc.: 95.31%] [G loss: 0.991313]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "525 [D loss: 2.049365, acc.: 28.12%] [G loss: 0.866217]\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "526 [D loss: 0.084875, acc.: 100.00%] [G loss: 1.474247]\n",
            "1/1 [==============================] - 0s 242ms/step\n",
            "527 [D loss: 1.843980, acc.: 20.31%] [G loss: 1.249191]\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "528 [D loss: 0.513414, acc.: 68.75%] [G loss: 1.086520]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "529 [D loss: 0.563305, acc.: 70.31%] [G loss: 1.175326]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "530 [D loss: 0.183758, acc.: 95.31%] [G loss: 0.752708]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "531 [D loss: 0.144270, acc.: 98.44%] [G loss: 1.003117]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "532 [D loss: 0.115029, acc.: 96.88%] [G loss: 1.237640]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "533 [D loss: 0.028043, acc.: 100.00%] [G loss: 0.906507]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "534 [D loss: 0.014552, acc.: 100.00%] [G loss: 0.727512]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "535 [D loss: 0.015898, acc.: 100.00%] [G loss: 0.747832]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "536 [D loss: 0.024192, acc.: 100.00%] [G loss: 0.828627]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "537 [D loss: 0.060207, acc.: 100.00%] [G loss: 0.686992]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "538 [D loss: 0.011407, acc.: 100.00%] [G loss: 0.613503]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "539 [D loss: 0.017807, acc.: 100.00%] [G loss: 0.735469]\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "540 [D loss: 0.080807, acc.: 98.44%] [G loss: 0.581057]\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "541 [D loss: 0.019882, acc.: 100.00%] [G loss: 0.806613]\n",
            "1/1 [==============================] - 0s 222ms/step\n",
            "542 [D loss: 0.016354, acc.: 100.00%] [G loss: 0.713800]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "543 [D loss: 0.025425, acc.: 100.00%] [G loss: 0.806906]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "544 [D loss: 0.071037, acc.: 100.00%] [G loss: 0.881523]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "545 [D loss: 0.089980, acc.: 100.00%] [G loss: 0.854238]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "546 [D loss: 0.202797, acc.: 92.19%] [G loss: 0.922424]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "547 [D loss: 0.761160, acc.: 57.81%] [G loss: 0.981409]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "548 [D loss: 0.184274, acc.: 96.88%] [G loss: 1.088587]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "549 [D loss: 0.426970, acc.: 78.12%] [G loss: 0.933487]\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "550 [D loss: 0.195874, acc.: 92.19%] [G loss: 1.024943]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "551 [D loss: 0.136429, acc.: 98.44%] [G loss: 0.996810]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "552 [D loss: 0.054084, acc.: 100.00%] [G loss: 1.001625]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "553 [D loss: 0.217567, acc.: 93.75%] [G loss: 1.249099]\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "554 [D loss: 1.138987, acc.: 39.06%] [G loss: 1.156425]\n",
            "1/1 [==============================] - 0s 217ms/step\n",
            "555 [D loss: 0.133663, acc.: 96.88%] [G loss: 1.608663]\n",
            "1/1 [==============================] - 0s 190ms/step\n",
            "556 [D loss: 0.138763, acc.: 96.88%] [G loss: 1.506414]\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "557 [D loss: 0.024923, acc.: 100.00%] [G loss: 0.799515]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "558 [D loss: 0.012417, acc.: 100.00%] [G loss: 1.114094]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "559 [D loss: 0.014648, acc.: 100.00%] [G loss: 0.924754]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "560 [D loss: 0.006074, acc.: 100.00%] [G loss: 0.767306]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "561 [D loss: 0.015910, acc.: 100.00%] [G loss: 0.763307]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "562 [D loss: 0.037589, acc.: 100.00%] [G loss: 0.630723]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "563 [D loss: 0.037580, acc.: 98.44%] [G loss: 0.688646]\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "564 [D loss: 0.168469, acc.: 93.75%] [G loss: 0.910207]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "565 [D loss: 0.049515, acc.: 100.00%] [G loss: 0.967561]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "566 [D loss: 0.027515, acc.: 100.00%] [G loss: 0.783754]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "567 [D loss: 0.144436, acc.: 95.31%] [G loss: 0.804972]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "568 [D loss: 0.435463, acc.: 76.56%] [G loss: 1.161566]\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "569 [D loss: 2.946906, acc.: 1.56%] [G loss: 0.879804]\n",
            "1/1 [==============================] - 0s 216ms/step\n",
            "570 [D loss: 0.330536, acc.: 87.50%] [G loss: 1.472425]\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "571 [D loss: 1.957335, acc.: 31.25%] [G loss: 1.109109]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "572 [D loss: 0.101410, acc.: 98.44%] [G loss: 0.934766]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "573 [D loss: 0.311668, acc.: 85.94%] [G loss: 0.828680]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "574 [D loss: 0.044339, acc.: 100.00%] [G loss: 0.705335]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "575 [D loss: 0.003725, acc.: 100.00%] [G loss: 0.630081]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "576 [D loss: 0.007239, acc.: 100.00%] [G loss: 0.874970]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "577 [D loss: 0.039132, acc.: 100.00%] [G loss: 1.005432]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "578 [D loss: 0.007399, acc.: 100.00%] [G loss: 1.016466]\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "579 [D loss: 0.003919, acc.: 100.00%] [G loss: 0.788857]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "580 [D loss: 0.005487, acc.: 100.00%] [G loss: 0.626155]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "581 [D loss: 0.006244, acc.: 100.00%] [G loss: 1.000840]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "582 [D loss: 0.043301, acc.: 100.00%] [G loss: 1.165437]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "583 [D loss: 0.010859, acc.: 100.00%] [G loss: 0.891356]\n",
            "1/1 [==============================] - 0s 211ms/step\n",
            "584 [D loss: 0.035271, acc.: 100.00%] [G loss: 0.679584]\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "585 [D loss: 0.014294, acc.: 100.00%] [G loss: 0.704626]\n",
            "1/1 [==============================] - 0s 214ms/step\n",
            "586 [D loss: 0.019778, acc.: 100.00%] [G loss: 0.600153]\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "587 [D loss: 0.012324, acc.: 100.00%] [G loss: 0.490991]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "588 [D loss: 0.020018, acc.: 100.00%] [G loss: 0.843701]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "589 [D loss: 0.039101, acc.: 100.00%] [G loss: 0.575665]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "590 [D loss: 0.068975, acc.: 100.00%] [G loss: 0.505036]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "591 [D loss: 0.032606, acc.: 100.00%] [G loss: 0.691941]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "592 [D loss: 0.451715, acc.: 76.56%] [G loss: 0.878228]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "593 [D loss: 0.196552, acc.: 93.75%] [G loss: 1.164350]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "594 [D loss: 0.089242, acc.: 100.00%] [G loss: 0.936438]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "595 [D loss: 0.347856, acc.: 85.94%] [G loss: 1.340516]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "596 [D loss: 0.369162, acc.: 81.25%] [G loss: 1.135273]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "597 [D loss: 0.949837, acc.: 51.56%] [G loss: 1.456393]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "598 [D loss: 0.463580, acc.: 67.19%] [G loss: 0.958091]\n",
            "1/1 [==============================] - 0s 223ms/step\n",
            "599 [D loss: 0.191718, acc.: 96.88%] [G loss: 1.028488]\n",
            "1/1 [==============================] - 0s 221ms/step\n",
            "600 [D loss: 0.066093, acc.: 98.44%] [G loss: 0.927528]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "601 [D loss: 0.020489, acc.: 100.00%] [G loss: 1.075119]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "602 [D loss: 0.227694, acc.: 93.75%] [G loss: 1.210352]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "603 [D loss: 0.072769, acc.: 100.00%] [G loss: 0.389754]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "604 [D loss: 0.002889, acc.: 100.00%] [G loss: 0.491599]\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "605 [D loss: 0.007100, acc.: 100.00%] [G loss: 0.604162]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "606 [D loss: 0.015166, acc.: 100.00%] [G loss: 0.504796]\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "607 [D loss: 0.042597, acc.: 100.00%] [G loss: 0.619627]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "608 [D loss: 0.010593, acc.: 100.00%] [G loss: 0.722288]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "609 [D loss: 0.047765, acc.: 100.00%] [G loss: 0.893462]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "610 [D loss: 0.017554, acc.: 100.00%] [G loss: 0.601930]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "611 [D loss: 0.016783, acc.: 100.00%] [G loss: 0.724056]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "612 [D loss: 0.115059, acc.: 98.44%] [G loss: 0.900950]\n",
            "1/1 [==============================] - 0s 203ms/step\n",
            "613 [D loss: 0.008331, acc.: 100.00%] [G loss: 0.894221]\n",
            "1/1 [==============================] - 0s 208ms/step\n",
            "614 [D loss: 0.194456, acc.: 93.75%] [G loss: 0.837471]\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "615 [D loss: 0.569078, acc.: 67.19%] [G loss: 0.705710]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "616 [D loss: 0.445715, acc.: 78.12%] [G loss: 0.746034]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "617 [D loss: 0.062798, acc.: 98.44%] [G loss: 0.792133]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "618 [D loss: 0.145787, acc.: 96.88%] [G loss: 0.970504]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "619 [D loss: 0.174868, acc.: 96.88%] [G loss: 1.028583]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "620 [D loss: 0.140077, acc.: 100.00%] [G loss: 0.904699]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "621 [D loss: 0.070665, acc.: 100.00%] [G loss: 1.147239]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "622 [D loss: 0.250102, acc.: 92.19%] [G loss: 1.109554]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "623 [D loss: 0.410242, acc.: 76.56%] [G loss: 0.960744]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "624 [D loss: 0.696308, acc.: 67.19%] [G loss: 0.810824]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "625 [D loss: 0.806742, acc.: 59.38%] [G loss: 1.093043]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "626 [D loss: 0.935280, acc.: 56.25%] [G loss: 1.009224]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "627 [D loss: 0.191297, acc.: 90.62%] [G loss: 0.775998]\n",
            "1/1 [==============================] - 0s 241ms/step\n",
            "628 [D loss: 0.036852, acc.: 100.00%] [G loss: 1.112163]\n",
            "1/1 [==============================] - 0s 211ms/step\n",
            "629 [D loss: 0.090560, acc.: 100.00%] [G loss: 1.260280]\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "630 [D loss: 1.513315, acc.: 29.69%] [G loss: 1.344088]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "631 [D loss: 0.104347, acc.: 98.44%] [G loss: 1.764242]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "632 [D loss: 0.138456, acc.: 95.31%] [G loss: 1.626116]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "633 [D loss: 0.193337, acc.: 93.75%] [G loss: 1.166592]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "634 [D loss: 0.047382, acc.: 98.44%] [G loss: 0.671634]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "635 [D loss: 0.064732, acc.: 100.00%] [G loss: 0.850013]\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "636 [D loss: 0.042467, acc.: 100.00%] [G loss: 0.598399]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "637 [D loss: 0.445221, acc.: 78.12%] [G loss: 0.987043]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "638 [D loss: 0.800381, acc.: 48.44%] [G loss: 0.884403]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "639 [D loss: 0.593829, acc.: 67.19%] [G loss: 0.945259]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "640 [D loss: 0.374611, acc.: 81.25%] [G loss: 1.078437]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "641 [D loss: 0.285329, acc.: 89.06%] [G loss: 0.869586]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "642 [D loss: 0.129634, acc.: 96.88%] [G loss: 0.952909]\n",
            "1/1 [==============================] - 0s 214ms/step\n",
            "643 [D loss: 0.371622, acc.: 82.81%] [G loss: 0.885084]\n",
            "1/1 [==============================] - 0s 220ms/step\n",
            "644 [D loss: 0.136375, acc.: 95.31%] [G loss: 0.937303]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "645 [D loss: 0.043466, acc.: 100.00%] [G loss: 1.235681]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "646 [D loss: 0.713638, acc.: 59.38%] [G loss: 0.634412]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "647 [D loss: 0.237293, acc.: 90.62%] [G loss: 0.803988]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "648 [D loss: 0.010195, acc.: 100.00%] [G loss: 1.386926]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "649 [D loss: 0.128742, acc.: 95.31%] [G loss: 1.836058]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "650 [D loss: 0.017168, acc.: 100.00%] [G loss: 1.482178]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "651 [D loss: 0.016444, acc.: 100.00%] [G loss: 1.404396]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "652 [D loss: 0.007117, acc.: 100.00%] [G loss: 1.213880]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "653 [D loss: 0.001053, acc.: 100.00%] [G loss: 1.224611]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "654 [D loss: 0.001703, acc.: 100.00%] [G loss: 0.965532]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "655 [D loss: 0.001572, acc.: 100.00%] [G loss: 1.183188]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "656 [D loss: 0.000735, acc.: 100.00%] [G loss: 1.368165]\n",
            "1/1 [==============================] - 0s 197ms/step\n",
            "657 [D loss: 0.001469, acc.: 100.00%] [G loss: 1.296299]\n",
            "1/1 [==============================] - 0s 204ms/step\n",
            "658 [D loss: 0.003286, acc.: 100.00%] [G loss: 1.487783]\n",
            "1/1 [==============================] - 0s 223ms/step\n",
            "659 [D loss: 0.004091, acc.: 100.00%] [G loss: 1.688401]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "660 [D loss: 0.002411, acc.: 100.00%] [G loss: 2.006557]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "661 [D loss: 0.001414, acc.: 100.00%] [G loss: 2.029341]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "662 [D loss: 0.017651, acc.: 100.00%] [G loss: 2.119010]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "663 [D loss: 0.002385, acc.: 100.00%] [G loss: 2.063269]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "664 [D loss: 0.000345, acc.: 100.00%] [G loss: 2.111680]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "665 [D loss: 0.000110, acc.: 100.00%] [G loss: 1.625666]\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "666 [D loss: 0.000810, acc.: 100.00%] [G loss: 1.712757]\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "667 [D loss: 0.001439, acc.: 100.00%] [G loss: 2.156641]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "668 [D loss: 0.001369, acc.: 100.00%] [G loss: 2.155069]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "669 [D loss: 0.002286, acc.: 100.00%] [G loss: 2.553964]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "670 [D loss: 0.000505, acc.: 100.00%] [G loss: 1.931472]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "671 [D loss: 0.000966, acc.: 100.00%] [G loss: 1.869553]\n",
            "1/1 [==============================] - 0s 207ms/step\n",
            "672 [D loss: 0.000426, acc.: 100.00%] [G loss: 1.551328]\n",
            "1/1 [==============================] - 0s 210ms/step\n",
            "673 [D loss: 0.000994, acc.: 100.00%] [G loss: 1.871732]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "674 [D loss: 0.000104, acc.: 100.00%] [G loss: 1.367739]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "675 [D loss: 0.000498, acc.: 100.00%] [G loss: 1.202357]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "676 [D loss: 0.000177, acc.: 100.00%] [G loss: 1.824335]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "677 [D loss: 0.000635, acc.: 100.00%] [G loss: 1.610188]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "678 [D loss: 0.001857, acc.: 100.00%] [G loss: 1.325798]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "679 [D loss: 0.000249, acc.: 100.00%] [G loss: 1.408730]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "680 [D loss: 0.000217, acc.: 100.00%] [G loss: 1.171587]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "681 [D loss: 0.001179, acc.: 100.00%] [G loss: 1.461627]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "682 [D loss: 0.000630, acc.: 100.00%] [G loss: 1.068070]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "683 [D loss: 0.000964, acc.: 100.00%] [G loss: 0.819786]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "684 [D loss: 0.000869, acc.: 100.00%] [G loss: 1.051313]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "685 [D loss: 0.000971, acc.: 100.00%] [G loss: 1.249010]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "686 [D loss: 0.001138, acc.: 100.00%] [G loss: 0.925455]\n",
            "1/1 [==============================] - 0s 201ms/step\n",
            "687 [D loss: 0.000684, acc.: 100.00%] [G loss: 0.906497]\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "688 [D loss: 0.000690, acc.: 100.00%] [G loss: 0.747403]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "689 [D loss: 0.001156, acc.: 100.00%] [G loss: 0.729062]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "690 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.590191]\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "691 [D loss: 0.000564, acc.: 100.00%] [G loss: 0.576236]\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "692 [D loss: 0.000317, acc.: 100.00%] [G loss: 0.601933]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "693 [D loss: 0.001313, acc.: 100.00%] [G loss: 0.817879]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "694 [D loss: 0.000619, acc.: 100.00%] [G loss: 0.518137]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "695 [D loss: 0.000823, acc.: 100.00%] [G loss: 0.458424]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "696 [D loss: 0.000355, acc.: 100.00%] [G loss: 0.524797]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "697 [D loss: 0.000757, acc.: 100.00%] [G loss: 0.645181]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "698 [D loss: 0.002586, acc.: 100.00%] [G loss: 0.603825]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "699 [D loss: 0.000842, acc.: 100.00%] [G loss: 0.691512]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "700 [D loss: 0.000656, acc.: 100.00%] [G loss: 0.700350]\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 201ms/step\n",
            "701 [D loss: 0.000431, acc.: 100.00%] [G loss: 0.340789]\n",
            "1/1 [==============================] - 0s 227ms/step\n",
            "702 [D loss: 0.002325, acc.: 100.00%] [G loss: 0.425488]\n",
            "1/1 [==============================] - 0s 214ms/step\n",
            "703 [D loss: 0.001564, acc.: 100.00%] [G loss: 0.662634]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "704 [D loss: 0.000805, acc.: 100.00%] [G loss: 0.474133]\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "705 [D loss: 0.004475, acc.: 100.00%] [G loss: 0.336598]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "706 [D loss: 0.001904, acc.: 100.00%] [G loss: 0.619045]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "707 [D loss: 0.004901, acc.: 100.00%] [G loss: 0.392842]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "708 [D loss: 0.015081, acc.: 100.00%] [G loss: 0.532245]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "709 [D loss: 0.001265, acc.: 100.00%] [G loss: 0.408201]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "710 [D loss: 0.007618, acc.: 100.00%] [G loss: 0.310532]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "711 [D loss: 0.003550, acc.: 100.00%] [G loss: 0.217885]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "712 [D loss: 0.002446, acc.: 100.00%] [G loss: 0.251472]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "713 [D loss: 0.007980, acc.: 100.00%] [G loss: 0.382919]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "714 [D loss: 0.000505, acc.: 100.00%] [G loss: 0.529488]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "715 [D loss: 0.001956, acc.: 100.00%] [G loss: 0.449163]\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "716 [D loss: 0.005272, acc.: 100.00%] [G loss: 0.554132]\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "717 [D loss: 0.001799, acc.: 100.00%] [G loss: 0.747658]\n",
            "1/1 [==============================] - 0s 197ms/step\n",
            "718 [D loss: 0.004109, acc.: 100.00%] [G loss: 0.894737]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "719 [D loss: 0.001363, acc.: 100.00%] [G loss: 1.116609]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "720 [D loss: 0.001183, acc.: 100.00%] [G loss: 0.696088]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "721 [D loss: 0.000547, acc.: 100.00%] [G loss: 0.712187]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "722 [D loss: 0.002160, acc.: 100.00%] [G loss: 1.042094]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "723 [D loss: 0.001620, acc.: 100.00%] [G loss: 0.958052]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "724 [D loss: 0.000694, acc.: 100.00%] [G loss: 1.034980]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "725 [D loss: 0.001150, acc.: 100.00%] [G loss: 0.947901]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "726 [D loss: 0.001262, acc.: 100.00%] [G loss: 1.100436]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "727 [D loss: 0.001896, acc.: 100.00%] [G loss: 1.060873]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "728 [D loss: 0.001354, acc.: 100.00%] [G loss: 1.490814]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "729 [D loss: 0.001400, acc.: 100.00%] [G loss: 1.188813]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "730 [D loss: 0.001084, acc.: 100.00%] [G loss: 1.595147]\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "731 [D loss: 0.002698, acc.: 100.00%] [G loss: 1.504728]\n",
            "1/1 [==============================] - 0s 217ms/step\n",
            "732 [D loss: 0.001195, acc.: 100.00%] [G loss: 1.226171]\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "733 [D loss: 0.000397, acc.: 100.00%] [G loss: 1.362702]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "734 [D loss: 0.001120, acc.: 100.00%] [G loss: 1.099885]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "735 [D loss: 0.000528, acc.: 100.00%] [G loss: 1.039710]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "736 [D loss: 0.000717, acc.: 100.00%] [G loss: 0.830279]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "737 [D loss: 0.001495, acc.: 100.00%] [G loss: 1.104671]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "738 [D loss: 0.000758, acc.: 100.00%] [G loss: 0.653954]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "739 [D loss: 0.000634, acc.: 100.00%] [G loss: 0.544825]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "740 [D loss: 0.004844, acc.: 100.00%] [G loss: 0.929445]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "741 [D loss: 0.005915, acc.: 100.00%] [G loss: 0.965515]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "742 [D loss: 0.012612, acc.: 100.00%] [G loss: 1.028303]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "743 [D loss: 0.000619, acc.: 100.00%] [G loss: 0.582778]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "744 [D loss: 0.005001, acc.: 100.00%] [G loss: 0.391542]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "745 [D loss: 0.001573, acc.: 100.00%] [G loss: 0.460329]\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "746 [D loss: 0.001134, acc.: 100.00%] [G loss: 0.321605]\n",
            "1/1 [==============================] - 0s 212ms/step\n",
            "747 [D loss: 0.001642, acc.: 100.00%] [G loss: 0.266902]\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "748 [D loss: 0.005083, acc.: 100.00%] [G loss: 0.301429]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "749 [D loss: 0.005272, acc.: 100.00%] [G loss: 0.185940]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "750 [D loss: 0.003342, acc.: 100.00%] [G loss: 0.377110]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "751 [D loss: 0.003800, acc.: 100.00%] [G loss: 0.428579]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "752 [D loss: 0.001380, acc.: 100.00%] [G loss: 0.420579]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "753 [D loss: 0.002350, acc.: 100.00%] [G loss: 0.509608]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "754 [D loss: 0.003667, acc.: 100.00%] [G loss: 0.607522]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "755 [D loss: 0.001776, acc.: 100.00%] [G loss: 0.560879]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "756 [D loss: 0.001207, acc.: 100.00%] [G loss: 0.760522]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "757 [D loss: 0.000679, acc.: 100.00%] [G loss: 0.545290]\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "758 [D loss: 0.002495, acc.: 100.00%] [G loss: 0.297549]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "759 [D loss: 0.001624, acc.: 100.00%] [G loss: 0.418001]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "760 [D loss: 0.015799, acc.: 100.00%] [G loss: 0.428083]\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 212ms/step\n",
            "761 [D loss: 0.002503, acc.: 100.00%] [G loss: 0.747772]\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "762 [D loss: 0.003704, acc.: 100.00%] [G loss: 0.363315]\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "763 [D loss: 0.008005, acc.: 100.00%] [G loss: 0.527119]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "764 [D loss: 0.005071, acc.: 100.00%] [G loss: 0.265593]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "765 [D loss: 0.005188, acc.: 100.00%] [G loss: 0.661969]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "766 [D loss: 0.006717, acc.: 100.00%] [G loss: 0.343811]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "767 [D loss: 0.014742, acc.: 100.00%] [G loss: 0.988220]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "768 [D loss: 0.006470, acc.: 100.00%] [G loss: 0.528207]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "769 [D loss: 0.033762, acc.: 100.00%] [G loss: 0.644445]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "770 [D loss: 0.036314, acc.: 100.00%] [G loss: 0.812165]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "771 [D loss: 0.003298, acc.: 100.00%] [G loss: 0.697298]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "772 [D loss: 0.016630, acc.: 100.00%] [G loss: 1.003194]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "773 [D loss: 0.040394, acc.: 100.00%] [G loss: 0.897212]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "774 [D loss: 0.012032, acc.: 100.00%] [G loss: 0.899431]\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "775 [D loss: 0.023657, acc.: 100.00%] [G loss: 1.088816]\n",
            "1/1 [==============================] - 0s 201ms/step\n",
            "776 [D loss: 0.014906, acc.: 100.00%] [G loss: 1.010474]\n",
            "1/1 [==============================] - 0s 230ms/step\n",
            "777 [D loss: 0.015544, acc.: 100.00%] [G loss: 0.921883]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "778 [D loss: 0.015316, acc.: 100.00%] [G loss: 0.529809]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "779 [D loss: 0.034639, acc.: 100.00%] [G loss: 0.622420]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "780 [D loss: 0.033190, acc.: 100.00%] [G loss: 0.748934]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "781 [D loss: 0.450697, acc.: 75.00%] [G loss: 1.162565]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "782 [D loss: 0.119462, acc.: 96.88%] [G loss: 1.516622]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "783 [D loss: 2.941657, acc.: 7.81%] [G loss: 0.951042]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "784 [D loss: 0.022273, acc.: 100.00%] [G loss: 1.729617]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "785 [D loss: 2.032790, acc.: 12.50%] [G loss: 1.191316]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "786 [D loss: 0.047621, acc.: 98.44%] [G loss: 2.213041]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "787 [D loss: 0.242745, acc.: 85.94%] [G loss: 2.230068]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "788 [D loss: 0.006482, acc.: 100.00%] [G loss: 2.308008]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "789 [D loss: 0.004001, acc.: 100.00%] [G loss: 1.997020]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "790 [D loss: 0.000218, acc.: 100.00%] [G loss: 1.791193]\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "791 [D loss: 0.002335, acc.: 100.00%] [G loss: 2.140361]\n",
            "1/1 [==============================] - 0s 221ms/step\n",
            "792 [D loss: 0.001246, acc.: 100.00%] [G loss: 2.776080]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "793 [D loss: 0.000258, acc.: 100.00%] [G loss: 2.608248]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "794 [D loss: 0.008151, acc.: 100.00%] [G loss: 3.618788]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "795 [D loss: 0.000215, acc.: 100.00%] [G loss: 2.583899]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "796 [D loss: 0.002081, acc.: 100.00%] [G loss: 3.196247]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "797 [D loss: 0.000657, acc.: 100.00%] [G loss: 3.559989]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "798 [D loss: 0.000301, acc.: 100.00%] [G loss: 2.786624]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "799 [D loss: 0.000629, acc.: 100.00%] [G loss: 2.743077]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "800 [D loss: 0.000281, acc.: 100.00%] [G loss: 2.049371]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "801 [D loss: 0.002486, acc.: 100.00%] [G loss: 3.280699]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "802 [D loss: 0.000483, acc.: 100.00%] [G loss: 2.696299]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "803 [D loss: 0.000649, acc.: 100.00%] [G loss: 2.490891]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "804 [D loss: 0.000983, acc.: 100.00%] [G loss: 2.373447]\n",
            "1/1 [==============================] - 0s 212ms/step\n",
            "805 [D loss: 0.001027, acc.: 100.00%] [G loss: 2.646832]\n",
            "1/1 [==============================] - 0s 197ms/step\n",
            "806 [D loss: 0.000664, acc.: 100.00%] [G loss: 2.502100]\n",
            "1/1 [==============================] - 0s 211ms/step\n",
            "807 [D loss: 0.000518, acc.: 100.00%] [G loss: 2.254468]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "808 [D loss: 0.000841, acc.: 100.00%] [G loss: 2.271930]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "809 [D loss: 0.000708, acc.: 100.00%] [G loss: 2.343523]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "810 [D loss: 0.000591, acc.: 100.00%] [G loss: 2.350960]\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "811 [D loss: 0.000442, acc.: 100.00%] [G loss: 1.726212]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "812 [D loss: 0.000274, acc.: 100.00%] [G loss: 1.356420]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "813 [D loss: 0.000349, acc.: 100.00%] [G loss: 1.399867]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "814 [D loss: 0.001035, acc.: 100.00%] [G loss: 1.812076]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "815 [D loss: 0.000811, acc.: 100.00%] [G loss: 1.484719]\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "816 [D loss: 0.000318, acc.: 100.00%] [G loss: 1.786851]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "817 [D loss: 0.000538, acc.: 100.00%] [G loss: 1.693892]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "818 [D loss: 0.000642, acc.: 100.00%] [G loss: 1.450165]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "819 [D loss: 0.000242, acc.: 100.00%] [G loss: 1.098378]\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "820 [D loss: 0.000486, acc.: 100.00%] [G loss: 1.196499]\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 209ms/step\n",
            "821 [D loss: 0.000505, acc.: 100.00%] [G loss: 1.060286]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "822 [D loss: 0.000345, acc.: 100.00%] [G loss: 0.925371]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "823 [D loss: 0.000826, acc.: 100.00%] [G loss: 0.732818]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "824 [D loss: 0.000904, acc.: 100.00%] [G loss: 0.742153]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "825 [D loss: 0.000881, acc.: 100.00%] [G loss: 0.699331]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "826 [D loss: 0.001716, acc.: 100.00%] [G loss: 0.533796]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "827 [D loss: 0.010988, acc.: 100.00%] [G loss: 0.338250]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "828 [D loss: 0.002927, acc.: 100.00%] [G loss: 0.300306]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "829 [D loss: 0.009715, acc.: 100.00%] [G loss: 0.348876]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "830 [D loss: 0.007741, acc.: 100.00%] [G loss: 0.498322]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "831 [D loss: 0.001138, acc.: 100.00%] [G loss: 0.537843]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "832 [D loss: 0.001734, acc.: 100.00%] [G loss: 0.501190]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "833 [D loss: 0.001487, acc.: 100.00%] [G loss: 0.579279]\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "834 [D loss: 0.000662, acc.: 100.00%] [G loss: 0.367785]\n",
            "1/1 [==============================] - 0s 203ms/step\n",
            "835 [D loss: 0.002403, acc.: 100.00%] [G loss: 0.574021]\n",
            "1/1 [==============================] - 0s 185ms/step\n",
            "836 [D loss: 0.000754, acc.: 100.00%] [G loss: 0.576859]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "837 [D loss: 0.000897, acc.: 100.00%] [G loss: 0.797896]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "838 [D loss: 0.001961, acc.: 100.00%] [G loss: 0.633643]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "839 [D loss: 0.000720, acc.: 100.00%] [G loss: 0.633369]\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "840 [D loss: 0.001216, acc.: 100.00%] [G loss: 0.671413]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "841 [D loss: 0.005824, acc.: 100.00%] [G loss: 0.682781]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "842 [D loss: 0.002419, acc.: 100.00%] [G loss: 0.463837]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "843 [D loss: 0.007020, acc.: 100.00%] [G loss: 0.595166]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "844 [D loss: 0.000913, acc.: 100.00%] [G loss: 0.679304]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "845 [D loss: 0.005047, acc.: 100.00%] [G loss: 0.623311]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "846 [D loss: 0.000930, acc.: 100.00%] [G loss: 0.572303]\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "847 [D loss: 0.007079, acc.: 100.00%] [G loss: 0.574003]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "848 [D loss: 0.004604, acc.: 100.00%] [G loss: 0.449649]\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "849 [D loss: 0.005180, acc.: 100.00%] [G loss: 0.443518]\n",
            "1/1 [==============================] - 0s 210ms/step\n",
            "850 [D loss: 0.001447, acc.: 100.00%] [G loss: 0.311942]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "851 [D loss: 0.021311, acc.: 100.00%] [G loss: 0.273006]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "852 [D loss: 0.001954, acc.: 100.00%] [G loss: 0.394016]\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "853 [D loss: 0.004274, acc.: 100.00%] [G loss: 0.351952]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "854 [D loss: 0.019001, acc.: 100.00%] [G loss: 0.358250]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "855 [D loss: 0.005702, acc.: 100.00%] [G loss: 0.392786]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "856 [D loss: 0.028530, acc.: 100.00%] [G loss: 0.481944]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "857 [D loss: 0.033004, acc.: 100.00%] [G loss: 0.733664]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "858 [D loss: 0.009904, acc.: 100.00%] [G loss: 0.708503]\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "859 [D loss: 0.030287, acc.: 100.00%] [G loss: 0.459911]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "860 [D loss: 0.071004, acc.: 98.44%] [G loss: 0.542550]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "861 [D loss: 0.030089, acc.: 100.00%] [G loss: 0.624176]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "862 [D loss: 0.531169, acc.: 70.31%] [G loss: 0.669566]\n",
            "1/1 [==============================] - 0s 258ms/step\n",
            "863 [D loss: 0.114205, acc.: 95.31%] [G loss: 0.408258]\n",
            "1/1 [==============================] - 0s 212ms/step\n",
            "864 [D loss: 0.042833, acc.: 100.00%] [G loss: 0.878495]\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "865 [D loss: 0.019961, acc.: 100.00%] [G loss: 0.845637]\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "866 [D loss: 0.059706, acc.: 100.00%] [G loss: 0.957838]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "867 [D loss: 0.039520, acc.: 100.00%] [G loss: 1.055543]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "868 [D loss: 0.037126, acc.: 100.00%] [G loss: 1.219527]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "869 [D loss: 0.047159, acc.: 100.00%] [G loss: 0.874246]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "870 [D loss: 0.301986, acc.: 89.06%] [G loss: 0.709623]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "871 [D loss: 0.019519, acc.: 100.00%] [G loss: 0.697223]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "872 [D loss: 0.008130, acc.: 100.00%] [G loss: 0.863872]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "873 [D loss: 0.040161, acc.: 100.00%] [G loss: 0.852201]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "874 [D loss: 0.003519, acc.: 100.00%] [G loss: 1.077813]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "875 [D loss: 0.009169, acc.: 100.00%] [G loss: 0.642467]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "876 [D loss: 0.014004, acc.: 100.00%] [G loss: 0.557134]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "877 [D loss: 0.005683, acc.: 100.00%] [G loss: 0.593648]\n",
            "1/1 [==============================] - 0s 212ms/step\n",
            "878 [D loss: 0.010066, acc.: 100.00%] [G loss: 0.788346]\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "879 [D loss: 0.049922, acc.: 100.00%] [G loss: 0.781031]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "880 [D loss: 0.022204, acc.: 100.00%] [G loss: 0.775769]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "881 [D loss: 0.033370, acc.: 98.44%] [G loss: 0.701825]\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "882 [D loss: 0.984571, acc.: 39.06%] [G loss: 1.140766]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "883 [D loss: 0.079298, acc.: 100.00%] [G loss: 2.090587]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "884 [D loss: 0.120343, acc.: 98.44%] [G loss: 1.803604]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "885 [D loss: 0.023053, acc.: 100.00%] [G loss: 2.092578]\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "886 [D loss: 0.001023, acc.: 100.00%] [G loss: 1.319092]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "887 [D loss: 0.001073, acc.: 100.00%] [G loss: 1.168542]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "888 [D loss: 0.009493, acc.: 100.00%] [G loss: 1.689495]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "889 [D loss: 0.002046, acc.: 100.00%] [G loss: 1.494191]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "890 [D loss: 0.001015, acc.: 100.00%] [G loss: 1.430076]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "891 [D loss: 0.025665, acc.: 100.00%] [G loss: 1.828184]\n",
            "1/1 [==============================] - 0s 225ms/step\n",
            "892 [D loss: 0.003510, acc.: 100.00%] [G loss: 1.546800]\n",
            "1/1 [==============================] - 0s 205ms/step\n",
            "893 [D loss: 0.001777, acc.: 100.00%] [G loss: 1.334888]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "894 [D loss: 0.003117, acc.: 100.00%] [G loss: 1.493339]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "895 [D loss: 0.002161, acc.: 100.00%] [G loss: 1.342243]\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "896 [D loss: 0.002092, acc.: 100.00%] [G loss: 1.226238]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "897 [D loss: 0.025194, acc.: 100.00%] [G loss: 0.695547]\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "898 [D loss: 0.004353, acc.: 100.00%] [G loss: 0.897826]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "899 [D loss: 0.003853, acc.: 100.00%] [G loss: 0.926417]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "900 [D loss: 0.002298, acc.: 100.00%] [G loss: 0.825881]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "901 [D loss: 0.021066, acc.: 100.00%] [G loss: 0.931542]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "902 [D loss: 0.008695, acc.: 100.00%] [G loss: 0.730072]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "903 [D loss: 0.052675, acc.: 100.00%] [G loss: 0.461357]\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "904 [D loss: 0.066311, acc.: 98.44%] [G loss: 0.701760]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "905 [D loss: 0.101134, acc.: 98.44%] [G loss: 1.069834]\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "906 [D loss: 0.119176, acc.: 98.44%] [G loss: 0.814255]\n",
            "1/1 [==============================] - 0s 217ms/step\n",
            "907 [D loss: 0.183566, acc.: 93.75%] [G loss: 0.756668]\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "908 [D loss: 0.819762, acc.: 56.25%] [G loss: 1.200043]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "909 [D loss: 0.230830, acc.: 93.75%] [G loss: 1.895851]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "910 [D loss: 1.544285, acc.: 21.88%] [G loss: 0.925355]\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "911 [D loss: 0.394676, acc.: 82.81%] [G loss: 1.019283]\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "912 [D loss: 1.006908, acc.: 53.12%] [G loss: 0.798311]\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "913 [D loss: 0.730566, acc.: 62.50%] [G loss: 0.626643]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "914 [D loss: 0.842598, acc.: 56.25%] [G loss: 0.557034]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "915 [D loss: 0.213555, acc.: 90.62%] [G loss: 0.380917]\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "916 [D loss: 0.036243, acc.: 100.00%] [G loss: 0.832780]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "917 [D loss: 0.036112, acc.: 98.44%] [G loss: 1.321286]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "918 [D loss: 0.017645, acc.: 100.00%] [G loss: 1.443859]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "919 [D loss: 0.007599, acc.: 100.00%] [G loss: 1.971635]\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "920 [D loss: 0.001438, acc.: 100.00%] [G loss: 1.980694]\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 220ms/step\n",
            "921 [D loss: 0.002544, acc.: 100.00%] [G loss: 1.906343]\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "922 [D loss: 0.011729, acc.: 100.00%] [G loss: 2.174835]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "923 [D loss: 0.001035, acc.: 100.00%] [G loss: 2.524610]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "924 [D loss: 0.024540, acc.: 100.00%] [G loss: 3.083498]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "925 [D loss: 0.002023, acc.: 100.00%] [G loss: 3.375411]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "926 [D loss: 0.004301, acc.: 100.00%] [G loss: 3.414070]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "927 [D loss: 0.005425, acc.: 100.00%] [G loss: 4.052535]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "928 [D loss: 0.002800, acc.: 100.00%] [G loss: 4.219666]\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "929 [D loss: 0.002785, acc.: 100.00%] [G loss: 4.717007]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "930 [D loss: 0.008642, acc.: 100.00%] [G loss: 4.704333]\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "931 [D loss: 0.000404, acc.: 100.00%] [G loss: 3.821853]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "932 [D loss: 0.000913, acc.: 100.00%] [G loss: 3.873583]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "933 [D loss: 0.002880, acc.: 100.00%] [G loss: 3.668474]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "934 [D loss: 0.004687, acc.: 100.00%] [G loss: 3.934673]\n",
            "1/1 [==============================] - 0s 224ms/step\n",
            "935 [D loss: 0.004010, acc.: 100.00%] [G loss: 3.682701]\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "936 [D loss: 0.000688, acc.: 100.00%] [G loss: 3.278873]\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "937 [D loss: 0.003434, acc.: 100.00%] [G loss: 3.486368]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "938 [D loss: 0.000886, acc.: 100.00%] [G loss: 2.791861]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "939 [D loss: 0.001861, acc.: 100.00%] [G loss: 2.773355]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "940 [D loss: 0.000313, acc.: 100.00%] [G loss: 2.900096]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "941 [D loss: 0.000456, acc.: 100.00%] [G loss: 1.828640]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "942 [D loss: 0.055144, acc.: 98.44%] [G loss: 2.576860]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "943 [D loss: 0.002058, acc.: 100.00%] [G loss: 2.104470]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "944 [D loss: 0.000607, acc.: 100.00%] [G loss: 1.138161]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "945 [D loss: 0.000321, acc.: 100.00%] [G loss: 0.873191]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "946 [D loss: 0.000583, acc.: 100.00%] [G loss: 0.502312]\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "947 [D loss: 0.009239, acc.: 100.00%] [G loss: 0.332871]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "948 [D loss: 0.006871, acc.: 100.00%] [G loss: 0.236185]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "949 [D loss: 0.001547, acc.: 100.00%] [G loss: 0.264240]\n",
            "1/1 [==============================] - 0s 222ms/step\n",
            "950 [D loss: 0.004956, acc.: 100.00%] [G loss: 0.157672]\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "951 [D loss: 0.010556, acc.: 100.00%] [G loss: 0.157256]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "952 [D loss: 0.010246, acc.: 100.00%] [G loss: 0.249111]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "953 [D loss: 0.039237, acc.: 98.44%] [G loss: 0.261441]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "954 [D loss: 0.003263, acc.: 100.00%] [G loss: 0.310968]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "955 [D loss: 0.010281, acc.: 100.00%] [G loss: 0.609529]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "956 [D loss: 0.003099, acc.: 100.00%] [G loss: 0.529574]\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "957 [D loss: 0.003859, acc.: 100.00%] [G loss: 0.928186]\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "958 [D loss: 0.000726, acc.: 100.00%] [G loss: 0.723878]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "959 [D loss: 0.002237, acc.: 100.00%] [G loss: 0.834137]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "960 [D loss: 0.002314, acc.: 100.00%] [G loss: 0.964628]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "961 [D loss: 0.002271, acc.: 100.00%] [G loss: 1.121099]\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "962 [D loss: 0.002366, acc.: 100.00%] [G loss: 1.354054]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "963 [D loss: 0.003238, acc.: 100.00%] [G loss: 1.002431]\n",
            "1/1 [==============================] - 0s 210ms/step\n",
            "964 [D loss: 0.002492, acc.: 100.00%] [G loss: 1.039279]\n",
            "1/1 [==============================] - 0s 201ms/step\n",
            "965 [D loss: 0.001826, acc.: 100.00%] [G loss: 0.804774]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "966 [D loss: 0.002159, acc.: 100.00%] [G loss: 0.714393]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "967 [D loss: 0.003812, acc.: 100.00%] [G loss: 0.858258]\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "968 [D loss: 0.000400, acc.: 100.00%] [G loss: 0.802369]\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "969 [D loss: 0.019649, acc.: 100.00%] [G loss: 0.870905]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "970 [D loss: 0.003810, acc.: 100.00%] [G loss: 0.880659]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "971 [D loss: 0.010760, acc.: 100.00%] [G loss: 0.675069]\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "972 [D loss: 0.023499, acc.: 100.00%] [G loss: 0.439260]\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "973 [D loss: 0.087522, acc.: 95.31%] [G loss: 0.435765]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "974 [D loss: 0.106104, acc.: 96.88%] [G loss: 0.659809]\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "975 [D loss: 0.080930, acc.: 100.00%] [G loss: 0.977036]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "976 [D loss: 1.121014, acc.: 46.88%] [G loss: 1.378427]\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "977 [D loss: 0.044514, acc.: 100.00%] [G loss: 3.831589]\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "978 [D loss: 1.751232, acc.: 50.00%] [G loss: 2.764373]\n",
            "1/1 [==============================] - 0s 205ms/step\n",
            "979 [D loss: 0.639107, acc.: 75.00%] [G loss: 1.916790]\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "980 [D loss: 0.017244, acc.: 100.00%] [G loss: 2.689485]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "981 [D loss: 0.144970, acc.: 92.19%] [G loss: 3.117577]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "982 [D loss: 0.005969, acc.: 100.00%] [G loss: 2.358149]\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "983 [D loss: 0.046113, acc.: 100.00%] [G loss: 1.559247]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "984 [D loss: 0.026156, acc.: 100.00%] [G loss: 0.971884]\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "985 [D loss: 0.061531, acc.: 100.00%] [G loss: 0.655958]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "986 [D loss: 1.157737, acc.: 50.00%] [G loss: 1.124101]\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "987 [D loss: 2.039019, acc.: 43.75%] [G loss: 1.038405]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "988 [D loss: 0.425742, acc.: 79.69%] [G loss: 0.910056]\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "989 [D loss: 0.314715, acc.: 85.94%] [G loss: 1.004489]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "990 [D loss: 0.035956, acc.: 100.00%] [G loss: 0.987775]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "991 [D loss: 0.002230, acc.: 100.00%] [G loss: 0.944934]\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "992 [D loss: 0.002908, acc.: 100.00%] [G loss: 1.163880]\n",
            "1/1 [==============================] - 0s 190ms/step\n",
            "993 [D loss: 0.026263, acc.: 100.00%] [G loss: 1.268038]\n",
            "1/1 [==============================] - 0s 209ms/step\n",
            "994 [D loss: 0.006257, acc.: 100.00%] [G loss: 2.263984]\n",
            "1/1 [==============================] - 0s 219ms/step\n",
            "995 [D loss: 0.002447, acc.: 100.00%] [G loss: 2.172823]\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "996 [D loss: 0.001156, acc.: 100.00%] [G loss: 2.461249]\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "997 [D loss: 0.009110, acc.: 100.00%] [G loss: 4.076608]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "998 [D loss: 0.004182, acc.: 100.00%] [G loss: 4.588981]\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "999 [D loss: 0.001055, acc.: 100.00%] [G loss: 4.383331]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_images1 = x_train[0:1500]\n",
        "print(len(mnist_images1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsLp86qztqE9",
        "outputId": "376461a6-169d-46be-cd82-3e8138fbfd1f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_path = \"/content/images\"\n",
        "mnist_images = os.listdir(mnist_path)"
      ],
      "metadata": {
        "id": "m_rdo3QkyCpb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnitimg_data=[]\n",
        "for img in mnist_images:\n",
        "    img_arr=cv2.imread(os.path.join(mnist_path,img))\n",
        "    mnitimg_data.append(img_arr)"
      ],
      "metadata": {
        "id": "1glgIT1_yWlJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_images2 = mnitimg_data[0:10]\n",
        "len(mnist_images2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xxz_xr3LydbE",
        "outputId": "b4acff35-ba83-4d06-cb14-f2017f212e80"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(mnitimg_data[40:41])):\n",
        "    plt.subplot(1,1,i+1)\n",
        "    plt.imshow(mnitimg_data[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "tCr9dWK9y0Cu",
        "outputId": "54e02d09-6b9b-41ff-ae02-0e4a8c8fa760"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#change the size\n",
        "def scale_images(images, new_shape):\n",
        "    images_list = list()\n",
        "    for image in images:\n",
        "        # resize with nearest neighbor interpolation\n",
        "        new_image = resize(image, new_shape, 0)\n",
        "        images_list.append(new_image)\n",
        "    return asarray(images_list)"
      ],
      "metadata": {
        "id": "qILne_3v0Cb1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the FID\n",
        "def calculate_fid(model, images1, images2):\n",
        "    # feature extraction\n",
        "    act1 = model.predict(images1)\n",
        "    act2 = model.predict(images2)\n",
        "    # calculate mean and covariance \n",
        "    mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
        "    mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
        "    # calculate sum squared difference between means\n",
        "    ssdiff = numpy.sum((mu1 - mu2)**2.0)\n",
        "    # calculate sqrt of product between cov\n",
        "    covmean = sqrtm(sigma1.dot(sigma2))\n",
        "    if iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "    # calculate score\n",
        "    fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "    return fid"
      ],
      "metadata": {
        "id": "8x5xrVqa0J3_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#inception model\n",
        "model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))"
      ],
      "metadata": {
        "id": "Et4BgzIZ0OiZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mnist_images1.shape)\n"
      ],
      "metadata": {
        "id": "AY7DXqif8ka2",
        "outputId": "15fb73d2-cc89-44d1-f937-c359dc905e75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1500, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# resize images\n",
        "mnist_images1 = scale_images(mnist_images1, (299,299,3))\n",
        "mnist_images2 = scale_images(mnist_images2, (299,299,3))\n",
        "print('Scaled', mnist_images1.shape, mnist_images2.shape)\n",
        "# pre-process images\n",
        "mnist_images1 = preprocess_input(mnist_images1)\n",
        "mnist_images2= preprocess_input(mnist_images2)"
      ],
      "metadata": {
        "id": "FPQkCjjlz-Ri"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mnist_images1.shape[0])\n",
        "print(mnist_images2.shape[0])"
      ],
      "metadata": {
        "id": "-BcRFNcy0YjC",
        "outputId": "deabfd98-2615-475b-a89e-2eae18de583c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the FID\n",
        "calculate_fid(model, mnist_images1, mnist_images2)"
      ],
      "metadata": {
        "id": "tucIMhGF4XQv",
        "outputId": "84b0b3c9-4a6d-4bba-b395-0fdd794fbee0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 203s 6s/step\n",
            "1/1 [==============================] - 2s 2s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59.5330208533327"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#images1 = 1000\n",
        "#images 2= 50"
      ],
      "metadata": {
        "id": "DTYO1vyc8AXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the FID\n",
        "calculate_fid(model, mnist_images1, mnist_images2)"
      ],
      "metadata": {
        "id": "iCYi3avx6oTP",
        "outputId": "70089f3e-8bfb-4a0c-b8c2-201d34d24dba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47/47 [==============================] - 302s 6s/step\n",
            "1/1 [==============================] - 2s 2s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59.47287987069939"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_fid(model, mnist_images1, mnist_images1[1:4999])"
      ],
      "metadata": {
        "id": "RhdkzLR48G2i",
        "outputId": "ce3eb808-a166-43c0-84fc-4b26ee36f397",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47/47 [==============================] - 307s 7s/step\n",
            "47/47 [==============================] - 306s 7s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00015355933418037665"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import floor\n",
        "from numpy import ones\n",
        "from numpy import expand_dims\n",
        "from numpy import log\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import exp\n",
        "from numpy.random import shuffle\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.datasets import cifar10\n",
        "from skimage.transform import resize\n",
        "from numpy import asarray\n",
        " "
      ],
      "metadata": {
        "id": "kL0KtHKkVUyQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Inception score\n",
        "\n",
        "def calculate_inception_score(images, n_split=10, eps=1E-16):\n",
        " # load inception v3 model\n",
        "    model = InceptionV3()\n",
        " # enumerate splits of images/predictions\n",
        "    scores = list()\n",
        "    n_part = floor(images.shape[0] / n_split)\n",
        "    for i in range(n_split):\n",
        " # retrieve images\n",
        "        ix_start, ix_end = i * n_part, (i+1) * n_part\n",
        "        subset = images[ix_start:ix_end]\n",
        " # convert from uint8 to float32\n",
        "        subset = subset.astype('float32')\n",
        " # scale images to the required size\n",
        "        subset = scale_images(subset, (299,299,3))\n",
        " # pre-process images, scale to [-1,1]\n",
        "        subset = preprocess_input(subset)\n",
        " # predict p(y|x)\n",
        "        p_yx = model.predict(subset)\n",
        " # calculate p(y)\n",
        "        p_y = expand_dims(p_yx.mean(axis=0), 0)\n",
        " # calculate KL divergence using log probabilities\n",
        "        kl_d = p_yx * (log(p_yx + eps) - log(p_y + eps))\n",
        " # sum over classes\n",
        "        sum_kl_d = kl_d.sum(axis=1)\n",
        " # average over images\n",
        "        avg_kl_d = mean(sum_kl_d)\n",
        " # undo the log\n",
        "        is_score = exp(avg_kl_d)\n",
        " # store\n",
        "        scores.append(is_score)\n",
        " # average across images\n",
        "        is_avg, is_std = mean(scores), std(scores)\n",
        "    return is_avg, is_std"
      ],
      "metadata": {
        "id": "z84UlljMUpEi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = mnitimg_data[0:50]\n",
        "# resize images\n",
        "images = scale_images(images, (299,299,3))\n",
        "#mnist_images2 = scale_images(mnist_images2, (299,299,3))\n",
        "print('Scaled', images.shape)\n",
        "# pre-process images\n",
        "images = preprocess_input(images)\n",
        "#mnist_images2= preprocess_input(mnist_images2)"
      ],
      "metadata": {
        "id": "cpkUAx98VrY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# calculate inception score\n",
        "is_avg, is_std = calculate_inception_score(images)\n",
        "print('score', is_avg, is_std)"
      ],
      "metadata": {
        "id": "BAKAC5hvU53w",
        "outputId": "48d07a18-b5b5-403c-fec3-c2d8c69ea09d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 11s 4s/step\n",
            "score 1.0000001 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(X_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "images1=X_train[1:1000]"
      ],
      "metadata": {
        "id": "NofeNFjdWz4Z"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resize images\n",
        "images1 = scale_images(images1, (299,299,3))\n",
        "#mnist_images2 = scale_images(mnist_images2, (299,299,3))\n",
        "print('Scaled', images1.shape)\n",
        "# pre-process images\n",
        "images1 = preprocess_input(images1)\n",
        "#mnist_images2= preprocess_input(mnist_images2)"
      ],
      "metadata": {
        "id": "fUGh6RIhXJ6O",
        "outputId": "8e889dfe-e511-449e-e948-a5acbb1ce213",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled (999, 299, 299, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# calculate inception score\n",
        "is_avg, is_std = calculate_inception_score(images1)\n",
        "print('score', is_avg, is_std)"
      ],
      "metadata": {
        "id": "7TEbksWTXsEo",
        "outputId": "0c6db297-5bd6-4be9-f25e-c98ab2a9a605",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 21s 4s/step\n",
            "4/4 [==============================] - 19s 4s/step\n",
            "4/4 [==============================] - 21s 4s/step\n",
            "4/4 [==============================] - 21s 4s/step\n",
            "4/4 [==============================] - 20s 5s/step\n",
            "4/4 [==============================] - 21s 5s/step\n",
            "4/4 [==============================] - 19s 5s/step\n",
            "4/4 [==============================] - 21s 4s/step\n",
            "4/4 [==============================] - 20s 4s/step\n",
            "4/4 [==============================] - 20s 4s/step\n",
            "score 1.0000002 1.1309186e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#images from drive\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "# Path to folder containing images\n",
        "#folder path\n",
        "path = \"/content/drive/MyDrive/animals/raw-img/scoiattolo\"\n"
      ],
      "metadata": {
        "id": "4DHv7G5tcQUU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = os.listdir(path)"
      ],
      "metadata": {
        "id": "d-d3ls4kcaa9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a list \n",
        "img_data=[]\n",
        "for img in images:\n",
        "    img_arr=cv2.imread(os.path.join(path,img))\n",
        "    img_data.append(img_arr)"
      ],
      "metadata": {
        "id": "FJx91HaDc28M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the len\n",
        "print(len(img_data))\n",
        "print(type(img_data))"
      ],
      "metadata": {
        "id": "6wTwtTAxdKqd",
        "outputId": "f9bf111a-c58e-449c-ca8d-9448559b72e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1863\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select the images for FID\n",
        "images_an = img_data[1:1500]\n",
        "#images2 = img_data[1:3]\n",
        "print(len(images_an))\n",
        "#print(len(images2))\n",
        "# convert integer to floating point values\n",
        "#images1 = float(files)\n",
        "#images2 = images2.astype('float32')\n"
      ],
      "metadata": {
        "id": "iwhw01c9dMVd",
        "outputId": "5528a1be-e5b6-4957-e277-ecaee73d259e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#resize and preprocess\n",
        "images_an = scale_images(images_an, (299,299,3))\n",
        "#images2 = scale_images(images2, (299,299,3))\n",
        "print('Scaled', images_an.shape)\n",
        "# pre-process images\n",
        "images_an = preprocess_input(images_an)\n",
        "#images2 = preprocess_input(images2)"
      ],
      "metadata": {
        "id": "_LExsrA6dYzr",
        "outputId": "7f04f4ed-de4b-4819-e82f-2acac9cd3b60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled (1499, 299, 299, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate inception score\n",
        "is_avg, is_std = calculate_inception_score(images_an)\n",
        "print('score', is_avg, is_std)"
      ],
      "metadata": {
        "id": "8qYof7lpfgHZ",
        "outputId": "ef3f4048-2a7c-446e-a208-bb3c93c24fcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 33s 7s/step\n",
            "5/5 [==============================] - 32s 6s/step\n",
            "5/5 [==============================] - 32s 6s/step\n",
            "5/5 [==============================] - 31s 6s/step\n",
            "5/5 [==============================] - 30s 6s/step\n",
            "5/5 [==============================] - 32s 6s/step\n",
            "5/5 [==============================] - 32s 6s/step\n",
            "5/5 [==============================] - 30s 6s/step\n",
            "5/5 [==============================] - 31s 6s/step\n",
            "5/5 [==============================] - 30s 6s/step\n",
            "score 1.0000002 1.1920929e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load cifar10 images\n",
        "(images, _), (_, _) = cifar10.load_data()\n",
        "# shuffle images\n",
        "#shuffle(images)"
      ],
      "metadata": {
        "id": "SU8CbqYiqPsp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = images[0:1500]"
      ],
      "metadata": {
        "id": "eLfwrvj9qStO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#resize and preprocess\n",
        "images= scale_images(images, (299,299,3))\n",
        "#images2 = scale_images(images2, (299,299,3))\n",
        "print('Scaled', images.shape)\n",
        "# pre-process images\n",
        "images= preprocess_input(images)\n",
        "#images2 = preprocess_input(images2)"
      ],
      "metadata": {
        "id": "sSxfOhoAqYdn",
        "outputId": "bb0aee6d-4ce0-422d-d52d-e0e2b57ec8ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled (1500, 299, 299, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate inception score\n",
        "is_avg, is_std = calculate_inception_score(images)\n",
        "print('score', is_avg, is_std)"
      ],
      "metadata": {
        "id": "ihqy_IAnqea8",
        "outputId": "7f70bb18-92bd-4c93-d0d6-85b092f162ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 32s 6s/step\n",
            "5/5 [==============================] - 31s 6s/step\n",
            "5/5 [==============================] - 32s 6s/step\n",
            "5/5 [==============================] - 31s 6s/step\n",
            "5/5 [==============================] - 33s 6s/step\n",
            "5/5 [==============================] - 31s 6s/step\n",
            "5/5 [==============================] - 31s 6s/step\n",
            "5/5 [==============================] - 31s 6s/step\n",
            "5/5 [==============================] - 31s 6s/step\n",
            "5/5 [==============================] - 32s 6s/step\n",
            "score 1.0000001 7.5394574e-08\n"
          ]
        }
      ]
    }
  ]
}